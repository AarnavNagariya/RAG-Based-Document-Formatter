{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Formatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current President of the United States is **Joe Biden**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "os.environ[\"API_KEY\"] = 'AIzaSyC8hiLFFM8_tH7B05QHZMXmSpHWgM0HdFU'\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "\n",
    "response = model.generate_content(\"President of USA is\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Analysis of the Paper: ‚ÄúCurriculum-Based Deep Reinforcement Learning for Quantum Control‚Äù\n",
      "\n",
      "## Index\n",
      "\n",
      "* **1. Overall Organization**\n",
      "    * 1.1 Introduction\n",
      "    * 1.2 Methodology\n",
      "    * 1.3 Results\n",
      "    * 1.4 Conclusion\n",
      "* **2. Organization within Sections**\n",
      "    * 2.1 Introduction\n",
      "    * 2.2 Methodology\n",
      "    * 2.3 Results\n",
      "* **3. Organization within Paragraphs**\n",
      "* **4. Quality of Figures**\n",
      "    * 4.1 Resolution\n",
      "    * 4.2 Font Sizes\n",
      "    * 4.3 Color Choices\n",
      "* **5. Lessons Learned from the Exercise**\n",
      "\n",
      "## 1. Overall Organization\n",
      "\n",
      "The paper is well-structured, with a clear introduction that sets the context for the problem of quantum control. It provides a comprehensive background on deep reinforcement learning (DRL), curriculum learning, and quantum control before delving into the proposed Curriculum-Based DRL (CDRL) methodology. The sections are logically organized, progressing from theory to methodology, followed by numerical results and a discussion of the findings. The conclusion ties back to the initial research goals, summarizing the advantages of the CDRL approach over traditional methods like genetic algorithms and gradient methods.\n",
      "\n",
      "### 1.1 Introduction\n",
      "\n",
      "The introduction sets the stage well, providing context on the challenges of quantum control and highlighting the motivation for using DRL. It logically transitions into the need for curriculum learning to improve the efficiency of DRL in quantum systems. The paragraph structure is cohesive, making the background clear to the reader.\n",
      "\n",
      "### 1.2 Methodology\n",
      "\n",
      "This section introduces the framework of CDRL and is quite dense with technical details. It begins with an overview of curriculum learning in the context of quantum systems, followed by an explanation of how tasks are sequenced based on fidelity thresholds. The paragraphs transition logically, but the depth of the content might make it difficult for less familiar readers to grasp the methodology without rereading. More interpretation or simplification could have helped in understanding how the tasks are generated dynamically.\n",
      "\n",
      "### 1.3 Results\n",
      "\n",
      "The results section is well-organized. The authors provide detailed comparisons of CDRL with other methods using numerical simulations on both closed and open quantum systems. Each subsection begins with context about the problem being addressed, followed by an interpretation of the results. The authors successfully explain the figures, making it clear how to interpret the results, though more emphasis could be placed on connecting the results to the broader implications for quantum control.\n",
      "\n",
      "### 1.4 Conclusion\n",
      "\n",
      "The conclusion summarizes the main findings and emphasizes the advantages of the CDRL approach. It highlights the improvements in performance and efficiency compared to traditional methods. The conclusion effectively ties back to the initial research goals and provides a clear roadmap for future research directions.\n",
      "\n",
      "## 2. Organization within Sections\n",
      "\n",
      "### 2.1 Introduction\n",
      "\n",
      "The introduction provides a compelling overview of the problem of quantum control and the limitations of traditional methods. The authors effectively convey the need for a new approach and introduce DRL as a potential solution. They then introduce curriculum learning as a key component of their proposed CDRL methodology. The introduction seamlessly connects the background information to the research problem and the proposed solution.\n",
      "\n",
      "### 2.2 Methodology\n",
      "\n",
      "The methodology section is technically dense and could benefit from further clarification for non-expert readers. The authors present a detailed description of the CDRL framework, but it could be more accessible with additional interpretive explanations. Breaking down the complex DRL algorithms, task generation, and reward structures into smaller, more manageable sections would improve comprehension. \n",
      "\n",
      "### 2.3 Results\n",
      "\n",
      "The results section effectively presents the numerical simulations and comparisons between CDRL and other methods. The authors clearly explain the experimental setups and interpret the results in a straightforward manner. They use figures and tables to visually illustrate the performance of different methods, making the results easily digestible. However, the section could benefit from a more in-depth discussion of the implications of the findings for the field of quantum control.\n",
      "\n",
      "## 3. Organization within Paragraphs\n",
      "\n",
      "The paragraphs generally follow a clear structure of context, content, and conclusions (CCC). For example:\n",
      "\n",
      "* In the Introduction, the authors start with the challenges of quantum control and proceed to explain why existing methods are insufficient, leading to the introduction of CDRL.\n",
      "* In the Results section, each simulation is introduced with an explanation of the setup, followed by the results and their interpretation. The authors successfully connect the results of different methods, such as CDRL and traditional DRL, but some paragraphs could have been broken down further to make the content easier to digest.\n",
      "\n",
      "One area where the paper could improve is in the Methodology section where the complexity of the DRL algorithms, task generation, and reward structures are presented in dense paragraphs. Breaking these down with more interpretive explanations could improve comprehension.\n",
      "\n",
      "## 4. Quality of Figures\n",
      "\n",
      "### 4.1 Resolution\n",
      "\n",
      "The figures are generally clear with adequate resolution, particularly the graphs comparing the performance of different algorithms (e.g., Fig. 4, Fig. 5). However, a few of the plots (such as Fig. 5) could benefit from a slightly larger font size to enhance readability, especially for readers looking at smaller screens.\n",
      "\n",
      "### 4.2 Font Sizes\n",
      "\n",
      "The font sizes for axis labels and legends are legible but could be slightly larger for figures with dense information, such as those comparing fidelity and rewards across episodes.\n",
      "\n",
      "### 4.3 Color Choices\n",
      "\n",
      "The color choices are appropriate and sufficiently distinguish between different methods. For example, the use of different shades and line styles in Fig. 5 helps distinguish between CDRL, DRL-1, and DRL-2.\n",
      "\n",
      "## 5. Lessons Learned from the Exercise\n",
      "\n",
      "This exercise highlighted the importance of clear transitions between sections and paragraphs, especially when dealing with complex topics such as deep reinforcement learning for quantum control. The authors did a good job of maintaining logical flow, but I was surprised by how dense some sections were, especially in the methodology. In future writing, I would focus on breaking down technical content into smaller, more digestible parts and ensuring that interpretations are presented immediately after introducing results. Additionally, I would pay more attention to figure readability by increasing font sizes and ensuring clear labeling. Overall, the exercise reinforced the importance of balancing technical depth with clarity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Analysis of the Paper: ‚ÄúCurriculum-Based Deep Reinforcement Learning for Quantum Control‚Äù\n",
    "1. Overall Organization:\n",
    "The paper is well-structured, with a clear introduction that sets the context for the problem of quantum control. It provides a comprehensive background on deep reinforcement learning (DRL), curriculum learning, and quantum control before delving into the proposed Curriculum-Based DRL (CDRL) methodology. The sections are logically organized, progressing from theory to methodology, followed by numerical results and a discussion of the findings. The conclusion ties back to the initial research goals, summarizing the advantages of the CDRL approach over traditional methods like genetic algorithms and gradient methods.\n",
    "\n",
    "2. Organization within Sections:\n",
    "Introduction: The introduction sets the stage well, providing context on the challenges of quantum control and highlighting the motivation for using DRL. It logically transitions into the need for curriculum learning to improve the efficiency of DRL in quantum systems. The paragraph structure is cohesive, making the background clear to the reader.\n",
    "\n",
    "Methodology (Section III): This section introduces the framework of CDRL and is quite dense with technical details. It begins with an overview of curriculum learning in the context of quantum systems, followed by an explanation of how tasks are sequenced based on fidelity thresholds. The paragraphs transition logically, but the depth of the content might make it difficult for less familiar readers to grasp the methodology without rereading. More interpretation or simplification could have helped in understanding how the tasks are generated dynamically.\n",
    "\n",
    "Results (Section IV): The results section is well-organized. The authors provide detailed comparisons of CDRL with other methods using numerical simulations on both closed and open quantum systems. Each subsection begins with context about the problem being addressed, followed by an interpretation of the results. The authors successfully explain the figures, making it clear how to interpret the results, though more emphasis could be placed on connecting the results to the broader implications for quantum control.\n",
    "\n",
    "3. Organization within Paragraphs:\n",
    "The paragraphs generally follow a clear structure of context, content, and conclusions (CCC). For example:\n",
    "\n",
    "In the Introduction, the authors start with the challenges of quantum control and proceed to explain why existing methods are insufficient, leading to the introduction of CDRL.\n",
    "In the Results section, each simulation is introduced with an explanation of the setup, followed by the results and their interpretation. The authors successfully connect the results of different methods, such as CDRL and traditional DRL, but some paragraphs could have been broken down further to make the content easier to digest.\n",
    "One area where the paper could improve is in the Methodology section where the complexity of the DRL algorithms, task generation, and reward structures are presented in dense paragraphs. Breaking these down with more interpretive explanations could improve comprehension.\n",
    "\n",
    "4. Quality of Figures:\n",
    "Resolution: The figures are generally clear with adequate resolution, particularly the graphs comparing the performance of different algorithms (e.g., Fig. 4, Fig. 5). However, a few of the plots (such as Fig. 5) could benefit from a slightly larger font size to enhance readability, especially for readers looking at smaller screens.\n",
    "Font Sizes: The font sizes for axis labels and legends are legible but could be slightly larger for figures with dense information, such as those comparing fidelity and rewards across episodes.\n",
    "Color Choices: The color choices are appropriate and sufficiently distinguish between different methods. For example, the use of different shades and line styles in Fig. 5 helps distinguish between CDRL, DRL-1, and DRL-2.\n",
    "5. Lessons Learned from the Exercise:\n",
    "This exercise highlighted the importance of clear transitions between sections and paragraphs, especially when dealing with complex topics such as deep reinforcement learning for quantum control. The authors did a good job of maintaining logical flow, but I was surprised by how dense some sections were, especially in the methodology. In future writing, I would focus on breaking down technical content into smaller, more digestible parts and ensuring that interpretations are presented immediately after introducing results. Additionally, I would pay more attention to figure readability by increasing font sizes and ensuring clear labeling. Overall, the exercise reinforced the importance of balancing technical depth with clarity.\n",
    "'''\n",
    "\n",
    "query = '''Your task is to covert the following text into \n",
    "markdown format with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "\n",
    "Also before this first write index in markdown as well with appropriate headings and subheadings.\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "\n",
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_string_to_markdown(input_string, file_name):\n",
    "    # Ensure the file name has a .md extension\n",
    "    if not file_name.endswith('.md'):\n",
    "        file_name += '.md'\n",
    "    \n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(file_name, 'w', encoding='utf-8') as md_file:\n",
    "        md_file.write(input_string)\n",
    "    \n",
    "    print(f\"String successfully saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Report_an.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_string_to_markdown(response.text, 'Report_an.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesRetrieval Augmented \n",
      "Generation (RAG)\n",
      "Umar Jamil\n",
      "Downloaded from: https://github.com/hkproj/retrieval -augmented -generation -notes\n",
      "License : Creative Commons Attribution -NonCommercial 4.0 International (CC BY -NC 4.0): \n",
      "https://creativecommons.org/licenses/by -nc/4.0/legalcode\n",
      "Not for commercial use\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Prerequisites\n",
      "‚Ä¢Structure of the Transformer model and how the attention mechanism works.\n",
      "‚Ä¢BERT  (MLM task, [ cls] token)\n",
      "Extra\n",
      "‚Ä¢As usual, my cat Â••Âà©Â•• will also be part of this video.\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWhat is a language model?\n",
      "A language model is a probabilistic model that assign probabilities to sequence of words.\n",
      "In practice, a language model allows us to compute the following:\n",
      "We usually train a neural network to predict these probabilities. A neural network trained on a \n",
      "large corpora of text is known as a Large Language Model (LLM).P [ ‚ÄúChina‚Äù | ‚ÄúShanghai is a city in‚Äù ]\n",
      "Prompt Next TokenUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesHow do we train and inference a Language Model?\n",
      "Training\n",
      "A language model is trained on a corpora of text, that is, a large collection of documents. Often, Language Models are \n",
      "trained on the entire Wikipedia and millions of web pages. This allows the Language Model to acquire as much knowledge \n",
      "as possible.\n",
      "We usually train a Transformer -based neural network as Language Model.\n",
      "Inference\n",
      "To inference a Language Model, we build a prompt and let the Language Model generate the rest by iteratively adding tokens.\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesYou are what you eat\n",
      "A language model can only output text and information that it was trained upon. This means, \n",
      "that if we train a language model only on English content, very probably it won‚Äôt be able to \n",
      "output Japanese or French. To teach new concepts, we need to fine -tune the model. Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesThe cons of fine -tuning\n",
      "‚Ä¢It can be expensive.\n",
      "‚Ä¢The number of parameters of the model may not be sufficient to capture all the knowledge \n",
      "we want to teach to it. That‚Äôs why LLaMA  was introduced with 7B, 13B and 70B parameters. \n",
      "‚Ä¢Fine -Tuning is not additive. It may replace existing knowledge of the model with new \n",
      "knowledge. For example, a language model trained on English that is (heavily) fine -tuned on \n",
      "Japanese may ‚Äúforget‚Äù English.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesPrompt Engineering to the rescue!\n",
      "It is possible to ‚Äúteach ‚Äù a language model how to perform a new task by playing with the \n",
      "prompt. For example, by using ‚Äúfew-shot ‚Äù prompting. The following is an example:\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesQA with Prompt Engineering\n",
      "Context\n",
      "AnswerQuestionPromptInstructionsUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesThe pros of fine -tuning\n",
      "‚Ä¢Higher quality results compared to prompt engineering.\n",
      "‚Ä¢Smaller context size (input size) during inference since we don ‚Äôt need to include the context \n",
      "and instructions.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesQA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWhy do we use vectors to represent words?\n",
      "Given the words ‚Äú cherry ‚Äù, ‚Äúdigital ‚Äù and ‚Äú information ‚Äù, if we represent the embedding vectors \n",
      "using only 2 dimensions (X, Y) and we plot them, we hope to see something like this: the angle \n",
      "between words with similar meaning is small, while the angle between words with different \n",
      "meaning is big. So, the embeddings ‚Äúcapture‚Äù the meaning of the words they represent by \n",
      "projecting them into a high -dimensional space.\n",
      "Source: Speech and Language Processing 3rd Edition Draft, Dan Jurafsky  and James H. Martin\n",
      "We commonly use the cosine similarity , which is based on the dot product between the two \n",
      "vectors.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWord embeddings: the ideas\n",
      "‚Ä¢Words that are synonyms tend to occur in the same context (surrounded by the same \n",
      "words). \n",
      "‚Ä¢For example, the word ‚Äúteacher ‚Äù and ‚Äúprofessor ‚Äù usually occur surrounded by the \n",
      "words ‚Äúschool ‚Äù,  ‚Äúuniversity ‚Äù, ‚Äúexam ‚Äù, ‚Äúlecture ‚Äù, ‚Äúcourse ‚Äù, etc..\n",
      "‚Ä¢The inverse can also be true: words that occur in the same context tend to have similar \n",
      "meanings. This is known as the distributional hypothesis .\n",
      "‚Ä¢This means that to capture the meaning of the word, we also need to have access to its \n",
      "context (the words surrounding it).\n",
      "‚Ä¢This is why we employ the Self -Attention mechanism in the Transformer model to capture \n",
      "contextual information for every token. The Self -Attention mechanism relates every token to \n",
      "all the other tokens in the sentence.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWord embeddings: the Cloze task\n",
      "‚Ä¢Imagine I give you the following sentence: \n",
      "Rome is the _______  of Italy, which is why it hosts many government buildings. \n",
      "Can you tell me what is the missing word?\n",
      "‚Ä¢Of course! The missing word is ‚Äúcapital ‚Äù, because by looking at the rest of the sentence, it is \n",
      "the one that makes the most sense.\n",
      "‚Ä¢This is how we train BERT: we want the Self -Attention mechanism to relate all the input \n",
      "tokens with each other, so that BERT has enough information about the ‚Äúcontext ‚Äù of the \n",
      "missing word to predict it.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesHow do we train embedding vectors in BERT?\n",
      "Input  (14 tokens):Output  (14 tokens):Target  (1 token): capital\n",
      "Run backpropagation  to update the weights Loss\n",
      "Rome is the [mask]  of Italy, which is why it hosts many government buildings.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13 TK14Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings\n",
      "‚Ä¢We can use the Self -Attention mechanism also to capture the ‚Äúmeaning‚Äù of an entire \n",
      "sentence.\n",
      "‚Ä¢We can use a pre -trained BERT model to produce embeddings of entire sentences. Let‚Äôs \n",
      "see howUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings with BERT\n",
      "Input  (13 tokens):Output  (13 tokens):\n",
      "Our professor always gives us lots of assignments to do in the weekend.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13\n",
      "Output of the Self -Attention : matrix of shape (13, 768)\n",
      "Input of the Self -Attention : matrix of shape (13, 768)Mean -Pooling: take the average of all the vectorsSentence EmbeddingSingle vector with 768 dimensionsUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings: comparison\n",
      "‚Ä¢How can we compare Sentence Embeddings to see if two sentences have similar \n",
      "‚Äúmeaning ‚Äù? We could use the cosine similarity, which measures the cosine of the angle \n",
      "between the two vectors. A small angle results in a high cosine similarity score.\n",
      "‚Ä¢But there‚Äôs a problem : nobody told BERT that the embeddings it produces should be \n",
      "comparable with the cosine similarity, that is, two similar sentences should be represented \n",
      "by vectors pointing to the same direction in space. How can we teach BERT to produce \n",
      "embeddings that can be compared with a similarity function of our choice?Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesIntroducing Sentence BERT\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence BERT: architecture\n",
      "Sentence A Sentence BBERTPoolingSentence Embedding A\n",
      "BERTPoolingSentence Embedding B\n",
      "Same architecture\n",
      "Same parameters\n",
      "Same weightsCosine SimilarityTarget  (Cosine Similarity)\n",
      "Loss (MSE)\n",
      "We can apply a linear layer to \n",
      "reduce the size of the \n",
      "embedding vector, for \n",
      "example from 768 to 512We can use mean -pooling, \n",
      "max -pooling or just use the [ cls] \n",
      "token as sentence embedding.Siamese Network\n",
      "‚ÄúMy father plays with my me at the park‚Äù ‚ÄúI play with my dad at the park ‚ÄùUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesQA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesStrategies to teach new concepts to LLM\n",
      "Base LLM\n",
      "Fine -tune on custom data\n",
      "Fine -tuned LLM+\n",
      "=Base LLM\n",
      "Vector DB + Embeddings\n",
      "RAG+\n",
      "=\n",
      "Fine -tuned LLM + RAGOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Vector DB: introduction\n",
      "A vector database stores vectors of fixed dimensions (called embeddings) such that we can then query the \n",
      "database to find all the embeddings that are closest (most similar) to a given query vector using a distance \n",
      "metric, which is usually the cosine similarity, but we can also use the Euclidean distance. The database uses \n",
      "a variant of the KNN (K Nearest Neighbor) algorithm or another similarity search algorithm. Vector DBs are \n",
      "also used for finding similar songs (e.g. Spotify), images (e.g. Google Images) or products (e.g. Amazon).\n",
      "Query\n",
      "Vector DBWeb Pages DocumentsHow many parameters are there in Grok -0?\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with 33 billion parameters. This early model approaches \n",
      "LLaMA  2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources. \n",
      "[...]Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KK-NN: a na√Øve approach\n",
      "Imagine we want to search for the query in our database: a simple way would be comparing \n",
      "the query with all the vectors, sorting them by distance, and keeping the top K.\n",
      "QueryHow many parameters are there in Grok -0?Emb . 1\n",
      "Emb . 2\n",
      "Emb . 3\n",
      "‚Ä¶\n",
      "Emb . 1,000,000‚Ä¶\n",
      "‚Ä¶\n",
      "‚Ä¶\n",
      "‚Ä¶\n",
      "Emb . 999,998\n",
      "Emb . 999,999Distance Vector\n",
      "0.3\n",
      "0.1\n",
      "0.2\n",
      "0.9\n",
      "0.2\n",
      "0.70.4\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.6Keep top KIf there are N embedding vectors and each \n",
      "has D dimensions, the computational \n",
      "complexity is in the order of O(N*D), too \n",
      "slow!üêåOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Similarity Search: let‚Äôs trade precision for speed\n",
      "The na√Øve approach we used before, always produces accurate \n",
      "results, since it compares the query with all the stored vectors, but \n",
      "what if we reduced the number of comparison, but still obtain \n",
      "accurate results with high probability?\n",
      "The metric we usually care about in Similarity Search is recall.\n",
      "In this video we will explore an algorithm for Approximate Nearest \n",
      "Neighbors, called Hierarchical Navigable Small Worlds (HNSW) . \n",
      "Source: WikipediaHNSW in the real world\n",
      "It is the same algorithm that powers Qdrant , the open source Vector DB used by Twitter‚Äôs  (X) Grok  LLM, \n",
      "which can access tweets in real time.\n",
      "HNSW: idea # 1\n",
      "HNSW is an evolution of the Navigable Small Worlds  algorithm for Approximate Nearest Neighbors, which \n",
      "is based on the concept of Six Degrees of Separation .\n",
      "Milgram's experiment aimed to test the social connections among people in the United States. The \n",
      "participants, who were initially located in Nebraska and Kansas, were given a letter to be delivered to a \n",
      "specific person in Boston. However, they were not allowed to send the letter directly to the recipient. \n",
      "Instead, they were instructed to send it to someone they knew on a first -name basis, who they believed \n",
      "might have a better chance of knowing the target person.\n",
      "At the end of Milgram ‚Äôs small -world experiment, Milgram found that most of the letters reached the final \n",
      "recipient in five or six steps, creating the concept that people all over the world are all connected by six \n",
      "degrees of separation.\n",
      "Facebook found in 2016 that its 1.59 billion active users were connected on average by 3.5 degrees of \n",
      "separation: https://research.facebook.com/blog/ 2016 /02/three -and-a-half-degrees -of-separation/\n",
      "This means that you and Mark Zuckerberg are only 3.5 connections apart!Navigable Small Worlds\n",
      "The NSW algorithm builds a graph that ‚Äì just like Facebook friends ‚Äì connects close vectors with each other \n",
      "but keeping the total number of connections small. For example, every vector may be connected to up to 6 \n",
      "other vectors (to mimic the Six Degrees of Separation).\n",
      "0102\n",
      "0305\n",
      "04\n",
      "0609\n",
      "0813\n",
      "11\n",
      "121510 07\n",
      "14Node Text\n",
      "01 [‚Ä¶] The Transformer is a model [‚Ä¶]\n",
      "02 [‚Ä¶] Diagnose cancer with AI [‚Ä¶]\n",
      "03 [‚Ä¶] A transformer -based model [‚Ä¶]\n",
      "04 [‚Ä¶] The Transformer has 6 layers [‚Ä¶]\n",
      "05 [‚Ä¶] An MRI machine that costs 1$ [‚Ä¶]\n",
      "06 [‚Ä¶] The dot -product is a [‚Ä¶]\n",
      "07 [‚Ä¶] Big -Pharma is not so big [‚Ä¶]\n",
      "08 [‚Ä¶] Cross -Attention is a great [‚Ä¶]\n",
      "09 [‚Ä¶] To solve an ODE [‚Ä¶]\n",
      "10 [‚Ä¶] We are aging too fast [‚Ä¶]\n",
      "11 [‚Ä¶] Open -source models like [‚Ä¶]\n",
      "12 [‚Ä¶] MathBERT : a new model [‚Ä¶]\n",
      "13 [‚Ä¶] AI to control aging [‚Ä¶]\n",
      "14 [‚Ä¶] Attention is all you need [‚Ä¶]\n",
      "15 [‚Ä¶] LLaMA  2 has 7B params [‚Ä¶]Navigable Small Worlds: searching for K -NN\n",
      "Given the following query: ‚Äú How many Encoder layers are there in the Transformer model? ‚Äù\n",
      "How does the algorithm find the K Nearest Neighbors?\n",
      "0102\n",
      "0305\n",
      "04\n",
      "0609\n",
      "0813\n",
      "11\n",
      "121510 07\n",
      "14Node Text\n",
      "01 [‚Ä¶] The Transformer is a model [‚Ä¶]\n",
      "02 [‚Ä¶] Diagnose cancer with AI [‚Ä¶]\n",
      "03 [‚Ä¶] A transformer -based model [‚Ä¶]\n",
      "04 [‚Ä¶] The Transformer has 6 layers [‚Ä¶]\n",
      "05 [‚Ä¶] An MRI machine that costs 1$ [‚Ä¶]\n",
      "06 [‚Ä¶] The dot -product is a [‚Ä¶]\n",
      "07 [‚Ä¶] Big -Pharma is not so big [‚Ä¶]\n",
      "08 [‚Ä¶] Cross -Attention is a great [‚Ä¶]\n",
      "09 [‚Ä¶] To solve an ODE [‚Ä¶]\n",
      "10 [‚Ä¶] We are aging too fast [‚Ä¶]\n",
      "11 [‚Ä¶] Open -source models like [‚Ä¶]\n",
      "12 [‚Ä¶] MathBERT : a new model [‚Ä¶]\n",
      "13 [‚Ä¶] AI to control aging [‚Ä¶]\n",
      "14 [‚Ä¶] Attention is all you need [‚Ä¶]\n",
      "15 [‚Ä¶] LLaMA  2 has 7B params [‚Ä¶]Q\n",
      "Local best: stop search\n",
      "We repeat the search with randomly chosen starting \n",
      "points and then keep the top K among all the visited \n",
      "nodes.Navigable Small Worlds: inserting a new vector\n",
      "We can insert a new vector by searching the top KNN with the searching algorithm described before and \n",
      "making an edge between the vector and the top K results.HNSW: idea #2\n",
      "To go from NSW (Navigable Small Worlds) to HNSW (Hierarchical Navigable Small Worlds), we need to \n",
      "introduce the algorithm behind the data structure known as Skip -List.\n",
      "The skip list is a data structure that maintains a sorted list and allows search and insertion with an average of \n",
      "ùëÇ(logùëÅ) time complexity.\n",
      "H3\n",
      "H2\n",
      "H1\n",
      "H0 EEEE\n",
      "1 5 3 9 12 17 23Let‚Äôs search the number 9HNSW: Hierarchical Navigable Small Worlds\n",
      "HNSW: Hierarchical Navigable Small Worlds\n",
      "Q\n",
      "Layer 0 (dense)Layer 3 (sparse)\n",
      "Layer 1Layer 2Let‚Äôs search!\n",
      "We repeat the search with \n",
      "randomly chosen starting \n",
      "points (on the top layer) and \n",
      "then keep the top K among \n",
      "all the visited nodes.QA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesThanks for watching!\n",
      "Don‚Äôt forget to subscribe for \n",
      "more amazing content on AI \n",
      "and Machine Learning!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        # Initialize a variable to store the extracted text\n",
    "        extracted_text = \"\"\n",
    "        \n",
    "        # Iterate through all the pages and extract text\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            extracted_text += page.extract_text()\n",
    "        \n",
    "        return extracted_text\n",
    "\n",
    "# Example usage\n",
    "pdf_file = 'RAG.pdf'  # Specify your PDF file path here\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Retrieval Augmented Generation (RAG)\n",
      "\n",
      "## Index\n",
      "\n",
      "### 1. Introduction\n",
      "   - What is a Language Model?\n",
      "   - How do we train and inference a Language Model?\n",
      "   - The cons of fine-tuning\n",
      "   - Prompt Engineering\n",
      "   - QA with Prompt Engineering\n",
      "   - The pros of fine-tuning\n",
      "   - QA with Retrieval Augmented Generation\n",
      "\n",
      "### 2. Embedding Vectors\n",
      "   - Why do we use vectors to represent words?\n",
      "   - Word embeddings: the ideas\n",
      "   - Word embeddings: the Cloze task\n",
      "   - How do we train embedding vectors in BERT?\n",
      "   - Sentence Embeddings\n",
      "   - Sentence Embeddings with BERT\n",
      "   - Sentence Embeddings: comparison\n",
      "   - Introducing Sentence BERT\n",
      "   - Sentence BERT: architecture\n",
      "   - Strategies to teach new concepts to LLM\n",
      "\n",
      "### 3. Vector DB\n",
      "   - Vector DB: introduction\n",
      "   - K-NN: a na√Øve approach\n",
      "   - Similarity Search: let's trade precision for speed\n",
      "   - HNSW in the real world\n",
      "   - HNSW: idea #1\n",
      "   - Navigable Small Worlds\n",
      "   - Navigable Small Worlds: searching for K-NN\n",
      "   - Navigable Small Worlds: inserting a new vector\n",
      "   - HNSW: idea #2\n",
      "   - HNSW: Hierarchical Navigable Small Worlds\n",
      "   - HNSW: Hierarchical Navigable Small Worlds\n",
      "\n",
      "## Retrieval Augmented Generation (RAG)\n",
      "\n",
      "### Intro to Large Language Models\n",
      "\n",
      "A language model is a probabilistic model that assigns probabilities to sequences of words. \n",
      "\n",
      "In practice, a language model allows us to compute the following: \n",
      "\n",
      "```\n",
      "P [ \"China\" | \"Shanghai is a city in\" ]\n",
      "```\n",
      "\n",
      "We usually train a neural network to predict these probabilities. A neural network trained on a large corpus of text is known as a Large Language Model (LLM). \n",
      "\n",
      "### RAG Pipeline\n",
      "\n",
      "The RAG pipeline consists of the following steps:\n",
      "\n",
      "1. **Split into chunks:** The input document is split into smaller chunks.\n",
      "2. **Embeddings:** Each chunk is converted into an embedding vector using Sentence BERT.\n",
      "3. **Store:** The embedding vectors are stored in a vector database.\n",
      "4. **Search:** When a query is given, its embedding vector is calculated and used to search the vector database for the most similar chunks.\n",
      "5. **Top-K:** The top-K most similar chunks are retrieved from the vector database.\n",
      "6. **LLM:** The retrieved chunks are fed into a large language model (LLM) along with the query.\n",
      "7. **Prompt:** The LLM generates a response based on the query and the retrieved context.\n",
      "\n",
      "### Embedding Vectors\n",
      "\n",
      "#### Why do we use vectors to represent words?\n",
      "\n",
      "Given the words \"cherry\", \"digital\" and \"information\", if we represent the embedding vectors using only 2 dimensions (X, Y) and we plot them, we hope to see something like this: the angle between words with similar meaning is small, while the angle between words with different meaning is big. So, the embeddings \"capture\" the meaning of the words they represent by projecting them into a high-dimensional space. \n",
      "\n",
      "We commonly use the cosine similarity, which is based on the dot product between the two vectors.\n",
      "\n",
      "#### Word embeddings: the ideas\n",
      "\n",
      "- Words that are synonyms tend to occur in the same context (surrounded by the same words). \n",
      "- For example, the word \"teacher\" and \"professor\" usually occur surrounded by the words \"school\", \"university\", \"exam\", \"lecture\", \"course\", etc..\n",
      "- The inverse can also be true: words that occur in the same context tend to have similar meanings. This is known as the **distributional hypothesis**.\n",
      "- This means that to capture the meaning of the word, we also need to have access to its context (the words surrounding it).\n",
      "- This is why we employ the Self-Attention mechanism in the Transformer model to capture contextual information for every token. The Self-Attention mechanism relates every token to all the other tokens in the sentence.\n",
      "\n",
      "#### Word embeddings: the Cloze task\n",
      "\n",
      "- Imagine I give you the following sentence: \n",
      "  Rome is the _______  of Italy, which is why it hosts many government buildings. \n",
      "  Can you tell me what is the missing word?\n",
      "- Of course! The missing word is \"capital\", because by looking at the rest of the sentence, it is the one that makes the most sense.\n",
      "- This is how we train BERT: we want the Self-Attention mechanism to relate all the input tokens with each other, so that BERT has enough information about the \"context\" of the missing word to predict it.\n",
      "\n",
      "#### How do we train embedding vectors in BERT?\n",
      "\n",
      "```\n",
      "Input  (14 tokens):  Output  (14 tokens):  Target  (1 token): capital\n",
      "Rome is the [mask]  of Italy, which is why it hosts many government buildings.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13 TK14\n",
      "```\n",
      "\n",
      "We run backpropagation to update the weights and minimize the loss.\n",
      "\n",
      "#### Sentence Embeddings\n",
      "\n",
      "We can use the Self-Attention mechanism also to capture the \"meaning\" of an entire sentence. We can use a pre-trained BERT model to produce embeddings of entire sentences. \n",
      "\n",
      "#### Sentence Embeddings with BERT\n",
      "\n",
      "```\n",
      "Input  (13 tokens):  Output  (13 tokens):\n",
      "Our professor always gives us lots of assignments to do in the weekend.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13\n",
      "```\n",
      "\n",
      "The output of the Self-Attention is a matrix of shape (13, 768). We can then apply mean-pooling to take the average of all the vectors, resulting in a single vector with 768 dimensions - our sentence embedding.\n",
      "\n",
      "#### Sentence Embeddings: comparison\n",
      "\n",
      "How can we compare Sentence Embeddings to see if two sentences have similar \"meaning\"? We could use the cosine similarity, which measures the cosine of the angle between the two vectors. A small angle results in a high cosine similarity score. \n",
      "\n",
      "But there's a problem: nobody told BERT that the embeddings it produces should be comparable with the cosine similarity, that is, two similar sentences should be represented by vectors pointing to the same direction in space. How can we teach BERT to produce embeddings that can be compared with a similarity function of our choice? \n",
      "\n",
      "### Introducing Sentence BERT\n",
      "\n",
      "Sentence BERT is a variation of BERT that is specifically trained to produce sentence embeddings that are comparable using cosine similarity. It does this by using a Siamese network.\n",
      "\n",
      "#### Sentence BERT: architecture\n",
      "\n",
      "Sentence BERT uses two identical BERT models, one for each input sentence. The outputs of the BERT models are then fed into a pooling layer to produce sentence embeddings. The sentence embeddings are then compared using cosine similarity, and the loss is calculated based on the difference between the cosine similarity and the target similarity score.\n",
      "\n",
      "```\n",
      "Sentence A\n",
      "BERT\n",
      "Pooling\n",
      "Sentence Embedding A\n",
      "Sentence B\n",
      "BERT\n",
      "Pooling\n",
      "Sentence Embedding B\n",
      "Cosine Similarity\n",
      "Target  (Cosine Similarity)\n",
      "Loss (MSE)\n",
      "```\n",
      "\n",
      "We can apply a linear layer to reduce the size of the embedding vector, for example from 768 to 512. We can use mean-pooling, max-pooling or just use the [cls] token as sentence embedding.\n",
      "\n",
      "#### Strategies to teach new concepts to LLM\n",
      "\n",
      "There are two main strategies for teaching new concepts to an LLM:\n",
      "\n",
      "1. **Fine-tuning:** The LLM is fine-tuned on a custom dataset that contains the desired knowledge. This can result in higher quality results compared to prompt engineering, but it can be expensive and may not be additive.\n",
      "2. **RAG:** The LLM is combined with a vector database that contains relevant context. This allows the LLM to access information from a wider range of sources, but it may not be as effective as fine-tuning for specific tasks.\n",
      "\n",
      "### Vector DB\n",
      "\n",
      "#### Vector DB: introduction\n",
      "\n",
      "A vector database stores vectors of fixed dimensions (called embeddings) such that we can then query the database to find all the embeddings that are closest (most similar) to a given query vector using a distance metric, which is usually the cosine similarity, but we can also use the Euclidean distance. The database uses a variant of the KNN (K Nearest Neighbor) algorithm or another similarity search algorithm. Vector DBs are also used for finding similar songs (e.g. Spotify), images (e.g. Google Images) or products (e.g. Amazon).\n",
      "\n",
      "#### K-NN: a na√Øve approach\n",
      "\n",
      "Imagine we want to search for the query in our database: a simple way would be comparing the query with all the vectors, sorting them by distance, and keeping the top K.\n",
      "\n",
      "If there are N embedding vectors and each has D dimensions, the computational complexity is in the order of O(N*D), too slow!üêå\n",
      "\n",
      "#### Similarity Search: let's trade precision for speed\n",
      "\n",
      "The na√Øve approach we used before, always produces accurate results, since it compares the query with all the stored vectors, but what if we reduced the number of comparison, but still obtain accurate results with high probability?\n",
      "\n",
      "The metric we usually care about in Similarity Search is recall.\n",
      "\n",
      "In this video we will explore an algorithm for Approximate Nearest Neighbors, called Hierarchical Navigable Small Worlds (HNSW). \n",
      "\n",
      "#### HNSW in the real world\n",
      "\n",
      "It is the same algorithm that powers Qdrant, the open source Vector DB used by Twitter‚Äôs (X) Grok LLM, which can access tweets in real time.\n",
      "\n",
      "#### HNSW: idea #1\n",
      "\n",
      "HNSW is an evolution of the Navigable Small Worlds algorithm for Approximate Nearest Neighbors, which is based on the concept of **Six Degrees of Separation**.\n",
      "\n",
      "Milgram's experiment aimed to test the social connections among people in the United States. The participants, who were initially located in Nebraska and Kansas, were given a letter to be delivered to a specific person in Boston. However, they were not allowed to send the letter directly to the recipient. Instead, they were instructed to send it to someone they knew on a first-name basis, who they believed might have a better chance of knowing the target person.\n",
      "\n",
      "At the end of Milgram‚Äôs small-world experiment, Milgram found that most of the letters reached the final recipient in five or six steps, creating the concept that people all over the world are all connected by six degrees of separation.\n",
      "\n",
      "Facebook found in 2016 that its 1.59 billion active users were connected on average by 3.5 degrees of separation: https://research.facebook.com/blog/2016/02/three-and-a-half-degrees-of-separation/\n",
      "\n",
      "This means that you and Mark Zuckerberg are only 3.5 connections apart!\n",
      "\n",
      "#### Navigable Small Worlds\n",
      "\n",
      "The NSW algorithm builds a graph that ‚Äì just like Facebook friends ‚Äì connects close vectors with each other but keeping the total number of connections small. For example, every vector may be connected to up to 6 other vectors (to mimic the Six Degrees of Separation).\n",
      "\n",
      "```\n",
      "01  02\n",
      "     \\\n",
      "      03\n",
      "     / \\\n",
      "    05  04\n",
      "   /  / \\\n",
      "  06  09  08\n",
      "       \\ /\n",
      "        13\n",
      "       / \\\n",
      "      11  12\n",
      "     /  \\  /\n",
      "    15  10 07\n",
      "        \\ /\n",
      "         14\n",
      "```\n",
      "\n",
      "#### Navigable Small Worlds: searching for K-NN\n",
      "\n",
      "Given the following query: ‚ÄúHow many Encoder layers are there in the Transformer model?‚Äù\n",
      "\n",
      "How does the algorithm find the K Nearest Neighbors?\n",
      "\n",
      "```\n",
      "01  02\n",
      "     \\\n",
      "      03\n",
      "     / \\\n",
      "    05  04\n",
      "   /  / \\\n",
      "  06  09  08\n",
      "       \\ /\n",
      "        13\n",
      "       / \\\n",
      "      11  12\n",
      "     /  \\  /\n",
      "    15  10 07\n",
      "        \\ /\n",
      "         14\n",
      "\n",
      "Node Text\n",
      "01 [‚Ä¶] The Transformer is a model [‚Ä¶]\n",
      "02 [‚Ä¶] Diagnose cancer with AI [‚Ä¶]\n",
      "03 [‚Ä¶] A transformer-based model [‚Ä¶]\n",
      "04 [‚Ä¶] The Transformer has 6 layers [‚Ä¶]\n",
      "05 [‚Ä¶] An MRI machine that costs 1$ [‚Ä¶]\n",
      "06 [‚Ä¶] The dot-product is a [‚Ä¶]\n",
      "07 [‚Ä¶] Big-Pharma is not so big [‚Ä¶]\n",
      "08 [‚Ä¶] Cross-Attention is a great [‚Ä¶]\n",
      "09 [‚Ä¶] To solve an ODE [‚Ä¶]\n",
      "10 [‚Ä¶] We are aging too fast [‚Ä¶]\n",
      "11 [‚Ä¶] Open-source models like [‚Ä¶]\n",
      "12 [‚Ä¶] MathBERT : a new model [‚Ä¶]\n",
      "13 [‚Ä¶] AI to control aging [‚Ä¶]\n",
      "14 [‚Ä¶] Attention is all you need [‚Ä¶]\n",
      "15 [‚Ä¶] LLaMA 2 has 7B params [‚Ä¶]\n",
      "```\n",
      "\n",
      "We start from a randomly chosen node and then we keep moving to the closest neighbor. \n",
      "\n",
      "#### Navigable Small Worlds: inserting a new vector\n",
      "\n",
      "We can insert a new vector by searching the top KNN with the searching algorithm described before and making an edge between the vector and the top K results.\n",
      "\n",
      "#### HNSW: idea #2\n",
      "\n",
      "To go from NSW (Navigable Small Worlds) to HNSW (Hierarchical Navigable Small Worlds), we need to introduce the algorithm behind the data structure known as Skip-List.\n",
      "\n",
      "The skip list is a data structure that maintains a sorted list and allows search and insertion with an average of ùëÇ(logùëÅ) time complexity.\n",
      "\n",
      "```\n",
      "H3\n",
      "H2\n",
      "H1\n",
      "H0 EEEE\n",
      "1 5 3 9 12 17 23\n",
      "```\n",
      "\n",
      "Let‚Äôs search the number 9\n",
      "\n",
      "#### HNSW: Hierarchical Navigable Small Worlds\n",
      "\n",
      "HNSW uses a hierarchical structure of graphs, where each level is sparser than the previous one.\n",
      "\n",
      "```\n",
      "Q\n",
      "Layer 0 (dense)\n",
      "Layer 1\n",
      "Layer 2\n",
      "Layer 3 (sparse)\n",
      "```\n",
      "\n",
      "Let‚Äôs search!\n",
      "\n",
      "We repeat the search with randomly chosen starting points (on the top layer) and then keep the top K among all the visited nodes.\n",
      "\n",
      "## Thanks for watching!\n",
      "\n",
      "Don‚Äôt forget to subscribe for more amazing content on AI and Machine Learning!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''Your task is to covert the following text into \n",
    "markdown format with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "\n",
    "Also before this first write index in markdown as well with appropriate headings and subheadings.\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Run_pdf.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_string_to_markdown(response.text, 'Run_pdf.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction The purpose of this book is t0 describe the optical structur and Oplical properties of the humnan eye_ It Will be useful [0 those who have an inleTeS[ in Vision. such as optometrists, ophthalmologists. vision scientists, Optical physicists. and sludents 0l visual optics.An understanding of the optics of the humaneye is panticularly important designers of ophthalmic diagnostic equipment and visual optical systems. such as telescopes_ Many animals have some SOrt of eye stnucture or sophisticated light sense Like humans. some rely heavily on vision. including predatory birds and insects such as honeybees and dragonflies- However; many animals rely much more on Other senses. particularly hearing and smell . than on vision_ The Visual sense iS Very complex and can process amounts of information Very rapidly How this is done is nOt fully understood: requires greater knowledge of how the neural components of vision (relina_ visual cortex, and other brain centers) process the retinal image. However. the first stage in this complex process is the fonation of the retinal image. In this [ext we investigale how the image is forined and discuss factors that affect its qualily- Most animal eyes can be divided into IWO groups: compound eyes (aS possessed by most insects) and verlebrale eyes (such as the human eye). Compared With Ver- tebrate eyes. there is considerable variation in the compound eyes. Compound eyes cOntain many optical elements (ommatidia). each with own aperture [0 the extemal world  Vertebrate eyes have single aperure [0 the extemal world. which is used by all the detectors. Several other animals have simple eyes which can be described developed versions of the vertebrate eye: All eyes- of whatever type . involve compromises between the need for detection (senSiIvily ) - panticularly at Iow light levels . and resolving capability in terms of the direction or formn of an object Although this book is about the of the human ‚Ç¨ye. we do not wish t0 con- sider the optics in complete isolation from the neural components. othenwvise We canol appreciate what influence changes in the retinal image will have on vision pertormance. As an example. altering the optics considerable influence on Teso- lution of objects for central vision but not for peripheral vision. This because the retina neural structure 1S fine enough at ItS center but not in the penphery- fOr large changes in optical quality to be of imponance (Chapter 18). Thus. the neural components of the visual system particularly the retinal detector. rate some mention in the book: The neural structures of the retina themselves produce optical effects-As an examnple the photoreceptors exhibit waveguide properties that make light ariving from directions more efficient at stimulating vision than light arriving from other directions Another example is that the regular arrangement of the nerve fiber layers produces polanization etiects. While image formnation in the eye is simnilar t0 that in man-made optical systems. such as cumeras and muSt theconventional optical Iaws thete   are   some interesting differences because of the eye biological basis. Perhaps the greatest diffetence iS that. as living organ. the eye responds t0 its environment. often t0 give the best image under different circumstances: Also. grows. suffers huge spatial oplics ha? SOle obey and AEC\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "def detect_text_with_easyocr(image_path):\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    results = reader.readtext(image_path)\n",
    "    \n",
    "    detected_text = ' '.join([result[1] for result in results])\n",
    "    return detected_text\n",
    "\n",
    "image_path = 'temp.png'\n",
    "text = detect_text_with_easyocr(image_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Index\n",
      "\n",
      "### I. Introduction\n",
      "\n",
      "#### 1.1. Purpose of the Book\n",
      "\n",
      "#### 1.2. Importance of Understanding Human Eye Optics\n",
      "\n",
      "#### 1.3. The Visual Sense in Different Animals\n",
      "\n",
      "#### 1.4. Image Formation and Its Quality\n",
      "\n",
      "#### 1.5. Types of Animal Eyes\n",
      "\n",
      "#### 1.6. Compromises in Eye Design\n",
      "\n",
      "#### 1.7. Importance of Neural Components\n",
      "\n",
      "#### 1.8. Optical Effects Produced by Retinal Structures\n",
      "\n",
      "#### 1.9. Differences between Human and Artificial Optical Systems\n",
      "\n",
      "---\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The purpose of this book is to describe the optical structure and optical properties of the human eye. It will be useful to those who have an interest in vision, such as optometrists, ophthalmologists, vision scientists, optical physicists, and students of visual optics. An understanding of the optics of the human eye is particularly important for designers of ophthalmic diagnostic equipment and visual optical systems, such as telescopes. \n",
      "\n",
      "Many animals have some sort of eye structure or sophisticated light sense. Like humans, some rely heavily on vision, including predatory birds and insects such as honeybees and dragonflies. However, many animals rely much more on other senses, particularly hearing and smell, than on vision. \n",
      "\n",
      "The visual sense is very complex and can process vast amounts of information very rapidly. How this is done is not fully understood; it requires greater knowledge of how the neural components of vision (retina, visual cortex, and other brain centers) process the retinal image. However, the first stage in this complex process is the formation of the retinal image. In this text, we investigate how the image is formed and discuss factors that affect its quality. \n",
      "\n",
      "Most animal eyes can be divided into two groups: compound eyes (as possessed by most insects) and vertebrate eyes (such as the human eye). Compared with vertebrate eyes, there is considerable variation in the compound eyes. Compound eyes contain many optical elements (ommatidia), each with its own aperture to the external world. Vertebrate eyes have a single aperture to the external world, which is used by all the detectors. Several other animals have simple eyes which can be described as developed versions of the vertebrate eye.\n",
      "\n",
      "All eyes, of whatever type, involve compromises between the need for detection (sensitivity), particularly at low light levels, and resolving capability in terms of the direction or form of an object. \n",
      "\n",
      "Although this book is about the optics of the human eye, we do not wish to consider the optics in complete isolation from the neural components. Otherwise, we cannot appreciate what influence changes in the retinal image will have on vision performance. As an example, altering the optics has considerable influence on the resolution of objects for central vision but not for peripheral vision. This is because the retina's neural structure is fine enough at its center but not in the periphery for large changes in optical quality to be of importance (Chapter 18). Thus, the neural components of the visual system, particularly the retinal detector, rate some mention in the book.\n",
      "\n",
      "The neural structures of the retina themselves produce optical effects. As an example, the photoreceptors exhibit waveguide properties that make light arriving from certain directions more efficient at stimulating vision than light arriving from other directions. Another example is that the regular arrangement of the nerve fiber layers produces polarization effects. \n",
      "\n",
      "While image formation in the eye is similar to that in man-made optical systems, such as cameras, and must obey the conventional optical laws, there are some interesting differences because of the eye's biological basis. Perhaps the greatest difference is that, as a living organ, the eye responds to its environment, often to give the best image under different circumstances. Also, it grows, suffers huge spatial optics, and has some ability to adapt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Run_image.md\n"
     ]
    }
   ],
   "source": [
    "save_string_to_markdown(response.text, 'Run_image.md')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
