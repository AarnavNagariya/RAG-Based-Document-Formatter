{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Donny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb+srv://iitpaanwallah:WS5AZQePfDphiRgQ@cluster0.oasgj.mongodb.net/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client[\"my_new_database\"]\n",
    "fs = gridfs.GridFS(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = db[\"my_new_collection\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Here are some of the traditional tests used to detect strabismus:\n",
    "●\n",
    "The Hirschberg Test involves shining a light into the patient's eyes and observing the reflection on the cornea. The position of the reflection can indicate the presence and magnitude of strabismus.1234> <56789 The corneal light reflex is not actually a reflection from the outside of the cornea. It is a reflection of Purkinje's image, which is a virtual image behind the pupil.10 The Hirschberg test is limited in that it can only detect aesthetic strabismus.11 There can be an apparent yaw (pseudostrabismus), where the pupillary axes are angled to each other but the visual axes are correctly positioned.11 Conversely, there can also be strabismus that is masked by a Kappa angle of the opposite sign, giving the appearance of proper binocular position despite a yaw.12\n",
    "●\n",
    "The Krimsky Test is a modification of the Hirschberg test. Prisms are placed in front of the fixing eye with the apex pointing in the direction of the deviation. This shifts the corneal reflex towards the centre of the pupil.13 The prism that centres the corneal reflex in both pupils indicates the angle of deviation.13 The Krimsky test is typically used on children or adults who cannot cooperate, those with sensory strabismus, or those with vision worse than 20/400.714\n",
    "●\n",
    "The Cover Test involves covering one eye and observing the movement of the uncovered eye. If the uncovered eye moves to fixate on a target, it indicates that the covered eye was not previously fixating on the target, signifying strabismus.151617 The cover test is the reference standard for detecting strabismus.1518 There are variations of the cover test, including:\n",
    "○\n",
    "The Cover-Uncover Test diagnoses manifest strabismus.16\n",
    "○\n",
    "The Alternate Cover Test diagnoses latent strabismus.19\n",
    "○\n",
    "The Simultaneous Prism Cover Test measures the magnitude of manifest and latent strabismus.19\n",
    "○\n",
    "The Prism Alternate Cover Test measures the magnitude of both manifest and latent strabismus.19\n",
    "●\n",
    "The Maddox Rod Test uses lenses that refract a point of light into a line image in front of each eye. The patient is asked to describe the orientation of the lines.3\n",
    "●\n",
    "The Double Maddox Rod (DMR) Test is the most common in-office test for the measurement of cyclodeviation.3 It is a subjective test.3\n",
    "●\n",
    "The Synoptophore projects two separate and dissimilar images into the same position in space to measure strabismus.2 Synoptophores are difficult to use for non-specialists. They are not compact or easily transported and can only be used on cooperative patients.20\n",
    "●\n",
    "The Lancaster Red-Green Test is a subjective test for strabismus.3\n",
    "The Bruckner test is not mentioned as being commonly used to test for strabismus in any of the sources. However, it is mentioned as being commonly used to detect amblyopia.21 The Bruckner test involves illuminating both eyes simultaneously from a distance of about one metre and noting any difference in the brightness of the fundus reflex.21 In the presence of strabismus, the reflex is darker in the fixing eye than in the deviated eye.21\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database and collection created, document inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "document = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"age\": 30,\n",
    "    \"text\": text\n",
    "}\n",
    "\n",
    "# Insert the document\n",
    "collection.insert_one(document)\n",
    "print(\"Database and collection created, document inserted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Alice.md\n",
      "PDF saved as StrabismusTest.pdf\n",
      "PDF uploaded successfully with file ID: 672e1a5f1209706260dba109\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import gridfs\n",
    "import markdown2\n",
    "from xhtml2pdf import pisa\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient(\"mongodb+srv://iitpaanwallah:WS5AZQePfDphiRgQ@cluster0.oasgj.mongodb.net/\")\n",
    "db = client[\"my_new_database\"]\n",
    "collection = db[\"my_new_collection\"]\n",
    "fs = gridfs.GridFS(db)\n",
    "\n",
    "for doc in collection.find():\n",
    "    text = doc['text']\n",
    "    name = doc['name']\n",
    "    \n",
    "    os.environ[\"API_KEY\"] = 'AIzaSyC8hiLFFM8_tH7B05QHZMXmSpHWgM0HdFU'\n",
    "    genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "\n",
    "    # query = '''Your task is to covert the following text into \n",
    "    # markdown format with appropriate headings, section etc.\n",
    "    # Make sure to do the appropriate formatting.\n",
    "\n",
    "    # Also before this first write index in markdown as well with appropriate headings and subheadings.\n",
    "    # Here is the text:\n",
    "    # '''\n",
    "\n",
    "\n",
    "    query = '''Your task is to covert the following text into \n",
    "    markdown format with appropriate headings, section etc.\n",
    "    Make sure to do the appropriate formatting.\n",
    "\n",
    "    Here is the text:\n",
    "    '''\n",
    "\n",
    "    response = model.generate_content(query + text)\n",
    "\n",
    "    def save_string_to_markdown(input_string, file_name):\n",
    "        # Ensure the file name has a .md extension\n",
    "        if not file_name.endswith('.md'):\n",
    "            file_name += '.md'\n",
    "        \n",
    "        # Open the file in write mode and write the string to it\n",
    "        with open(file_name, 'w', encoding='utf-8') as md_file:\n",
    "            md_file.write(input_string)\n",
    "        \n",
    "        print(f\"String successfully saved to {file_name}\")\n",
    "\n",
    "\n",
    "    save_string_to_markdown(response.text, name + '.md')\n",
    "\n",
    "    def add_header_id(html_content):\n",
    "            # Use a regular expression to find the first <h1> tag and add an id=\"header_content\"\n",
    "            html_with_id = re.sub(r'(<h1\\b[^>]*)(>)', r'\\1 id=\"header_content\"\\2', html_content, count=1)\n",
    "            return html_with_id\n",
    "    def add_footer(html_content):\n",
    "        html_with_footer = html_content + '''<div id=\"footer_content\">(c) - page <pdf:pagenumber> of <pdf:pagecount></div>'''\n",
    "        return html_with_footer\n",
    "    def markdown_to_pdf(md_file, css_file, output_pdf):\n",
    "        with open(md_file, 'r', encoding='utf-8') as file:\n",
    "            md_content = file.read()\n",
    "\n",
    "        html_content = markdown2.markdown(md_content)\n",
    "        html_content = add_header_id(html_content) #only affects css with header_content id\n",
    "        html_content = add_footer(html_content) #only affects css with footer_content id\n",
    "        with open(css_file, 'r', encoding='utf-8') as file:\n",
    "            css_content = file.read()\n",
    "        \n",
    "        styled_html = f\"\"\"\n",
    "        <html>\n",
    "        <head>\n",
    "        <style>{css_content}</style>\n",
    "        </head>\n",
    "        <body>\n",
    "        {html_content}\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        with open(output_pdf, 'wb') as pdf_file:\n",
    "            pisa_status = pisa.CreatePDF(styled_html, dest=pdf_file)\n",
    "\n",
    "        if pisa_status.err:\n",
    "            print(f\"Error during PDF generation: {pisa_status.err}\")\n",
    "        else:\n",
    "            print(f\"PDF saved as {output_pdf}\")\n",
    "\n",
    "\n",
    "    markdown_file = 'Workflow.md'\n",
    "    css_file = 'style/style2_copy.css'\n",
    "    output_pdf = name + '.pdf'\n",
    "\n",
    "    markdown_to_pdf(markdown_file, css_file, output_pdf)\n",
    "\n",
    "    with open(output_pdf, \"rb\") as f:\n",
    "        file_id = fs.put(f, filename= output_pdf)\n",
    "        print(f\"PDF uploaded successfully with file ID: {file_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.delete(file_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current President of the United States of America is **Joe Biden**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "os.environ[\"API_KEY\"] = 'AIzaSyC8hiLFFM8_tH7B05QHZMXmSpHWgM0HdFU'\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "\n",
    "response = model.generate_content(\"President of USA is\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Index\n",
      "\n",
      "### Traditional Tests for Strabismus Detection\n",
      "\n",
      "* **Hirschberg Test** \n",
      "    * Description\n",
      "    * Limitations\n",
      "    * Pseudostrabismus\n",
      "* **Krimsky Test**\n",
      "    * Description\n",
      "    * Use Cases\n",
      "* **Cover Test**\n",
      "    * Description\n",
      "    * Variations\n",
      "        * Cover-Uncover Test\n",
      "        * Alternate Cover Test\n",
      "        * Simultaneous Prism Cover Test\n",
      "        * Prism Alternate Cover Test\n",
      "* **Maddox Rod Test** \n",
      "* **Double Maddox Rod (DMR) Test**\n",
      "* **Synoptophore**\n",
      "    * Description\n",
      "    * Limitations\n",
      "* **Lancaster Red-Green Test** \n",
      "* **Bruckner Test** \n",
      "    * Description\n",
      "    * Use Case\n",
      "\n",
      "## Traditional Tests for Strabismus Detection\n",
      "\n",
      "### Hirschberg Test\n",
      "\n",
      "The Hirschberg Test involves shining a light into the patient's eyes and observing the reflection on the cornea.1234> <56789 The corneal light reflex is not actually a reflection from the outside of the cornea. It is a reflection of Purkinje's image, which is a virtual image behind the pupil.10 The Hirschberg test is limited in that it can only detect aesthetic strabismus.11 There can be an apparent yaw (pseudostrabismus), where the pupillary axes are angled to each other but the visual axes are correctly positioned.11 Conversely, there can also be strabismus that is masked by a Kappa angle of the opposite sign, giving the appearance of proper binocular position despite a yaw.12\n",
      "\n",
      "### Krimsky Test\n",
      "\n",
      "The Krimsky Test is a modification of the Hirschberg test. Prisms are placed in front of the fixing eye with the apex pointing in the direction of the deviation. This shifts the corneal reflex towards the centre of the pupil.13 The prism that centres the corneal reflex in both pupils indicates the angle of deviation.13 The Krimsky test is typically used on children or adults who cannot cooperate, those with sensory strabismus, or those with vision worse than 20/400.714\n",
      "\n",
      "### Cover Test\n",
      "\n",
      "The Cover Test involves covering one eye and observing the movement of the uncovered eye. If the uncovered eye moves to fixate on a target, it indicates that the covered eye was not previously fixating on the target, signifying strabismus.151617 The cover test is the reference standard for detecting strabismus.1518 \n",
      "\n",
      "#### Variations of the Cover Test\n",
      "\n",
      "* **Cover-Uncover Test:** Diagnoses manifest strabismus.16\n",
      "* **Alternate Cover Test:** Diagnoses latent strabismus.19\n",
      "* **Simultaneous Prism Cover Test:** Measures the magnitude of manifest and latent strabismus.19\n",
      "* **Prism Alternate Cover Test:** Measures the magnitude of both manifest and latent strabismus.19\n",
      "\n",
      "### Maddox Rod Test\n",
      "\n",
      "The Maddox Rod Test uses lenses that refract a point of light into a line image in front of each eye. The patient is asked to describe the orientation of the lines.3\n",
      "\n",
      "### Double Maddox Rod (DMR) Test\n",
      "\n",
      "The Double Maddox Rod (DMR) Test is the most common in-office test for the measurement of cyclodeviation.3 It is a subjective test.3\n",
      "\n",
      "### Synoptophore\n",
      "\n",
      "The Synoptophore projects two separate and dissimilar images into the same position in space to measure strabismus.2 Synoptophores are difficult to use for non-specialists. They are not compact or easily transported and can only be used on cooperative patients.20\n",
      "\n",
      "### Lancaster Red-Green Test\n",
      "\n",
      "The Lancaster Red-Green Test is a subjective test for strabismus.3\n",
      "\n",
      "### Bruckner Test\n",
      "\n",
      "The Bruckner test is not mentioned as being commonly used to test for strabismus in any of the sources. However, it is mentioned as being commonly used to detect amblyopia.21 The Bruckner test involves illuminating both eyes simultaneously from a distance of about one metre and noting any difference in the brightness of the fundus reflex.21 In the presence of strabismus, the reflex is darker in the fixing eye than in the deviated eye.21 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Here are some of the traditional tests used to detect strabismus:\n",
    "●\n",
    "The Hirschberg Test involves shining a light into the patient's eyes and observing the reflection on the cornea. The position of the reflection can indicate the presence and magnitude of strabismus.1234> <56789 The corneal light reflex is not actually a reflection from the outside of the cornea. It is a reflection of Purkinje's image, which is a virtual image behind the pupil.10 The Hirschberg test is limited in that it can only detect aesthetic strabismus.11 There can be an apparent yaw (pseudostrabismus), where the pupillary axes are angled to each other but the visual axes are correctly positioned.11 Conversely, there can also be strabismus that is masked by a Kappa angle of the opposite sign, giving the appearance of proper binocular position despite a yaw.12\n",
    "●\n",
    "The Krimsky Test is a modification of the Hirschberg test. Prisms are placed in front of the fixing eye with the apex pointing in the direction of the deviation. This shifts the corneal reflex towards the centre of the pupil.13 The prism that centres the corneal reflex in both pupils indicates the angle of deviation.13 The Krimsky test is typically used on children or adults who cannot cooperate, those with sensory strabismus, or those with vision worse than 20/400.714\n",
    "●\n",
    "The Cover Test involves covering one eye and observing the movement of the uncovered eye. If the uncovered eye moves to fixate on a target, it indicates that the covered eye was not previously fixating on the target, signifying strabismus.151617 The cover test is the reference standard for detecting strabismus.1518 There are variations of the cover test, including:\n",
    "○\n",
    "The Cover-Uncover Test diagnoses manifest strabismus.16\n",
    "○\n",
    "The Alternate Cover Test diagnoses latent strabismus.19\n",
    "○\n",
    "The Simultaneous Prism Cover Test measures the magnitude of manifest and latent strabismus.19\n",
    "○\n",
    "The Prism Alternate Cover Test measures the magnitude of both manifest and latent strabismus.19\n",
    "●\n",
    "The Maddox Rod Test uses lenses that refract a point of light into a line image in front of each eye. The patient is asked to describe the orientation of the lines.3\n",
    "●\n",
    "The Double Maddox Rod (DMR) Test is the most common in-office test for the measurement of cyclodeviation.3 It is a subjective test.3\n",
    "●\n",
    "The Synoptophore projects two separate and dissimilar images into the same position in space to measure strabismus.2 Synoptophores are difficult to use for non-specialists. They are not compact or easily transported and can only be used on cooperative patients.20\n",
    "●\n",
    "The Lancaster Red-Green Test is a subjective test for strabismus.3\n",
    "The Bruckner test is not mentioned as being commonly used to test for strabismus in any of the sources. However, it is mentioned as being commonly used to detect amblyopia.21 The Bruckner test involves illuminating both eyes simultaneously from a distance of about one metre and noting any difference in the brightness of the fundus reflex.21 In the presence of strabismus, the reflex is darker in the fixing eye than in the deviated eye.21\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "query = '''Your task is to covert the following text into \n",
    "markdown format with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "\n",
    "Also before this first write index in markdown as well with appropriate headings and subheadings.\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "\n",
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_string_to_markdown(input_string, file_name):\n",
    "    # Ensure the file name has a .md extension\n",
    "    if not file_name.endswith('.md'):\n",
    "        file_name += '.md'\n",
    "    \n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(file_name, 'w', encoding='utf-8') as md_file:\n",
    "        md_file.write(input_string)\n",
    "    \n",
    "    print(f\"String successfully saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Workflow.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_string_to_markdown(response.text, 'Workflow.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesRetrieval Augmented \n",
      "Generation (RAG)\n",
      "Umar Jamil\n",
      "Downloaded from: https://github.com/hkproj/retrieval -augmented -generation -notes\n",
      "License : Creative Commons Attribution -NonCommercial 4.0 International (CC BY -NC 4.0): \n",
      "https://creativecommons.org/licenses/by -nc/4.0/legalcode\n",
      "Not for commercial use\n",
      "Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "•Intro to Large Language Models\n",
      "•RAG pipeline\n",
      "•Embedding vectors\n",
      "•Sentence BERT\n",
      "•Vector DB\n",
      "•Algorithms (HNSW)Prerequisites\n",
      "•Structure of the Transformer model and how the attention mechanism works.\n",
      "•BERT  (MLM task, [ cls] token)\n",
      "Extra\n",
      "•As usual, my cat 奥利奥 will also be part of this video.\n",
      "Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "•Intro to Large Language Models\n",
      "•RAG pipeline\n",
      "•Embedding vectors\n",
      "•Sentence BERT\n",
      "•Vector DB\n",
      "•Algorithms (HNSW)Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesWhat is a language model?\n",
      "A language model is a probabilistic model that assign probabilities to sequence of words.\n",
      "In practice, a language model allows us to compute the following:\n",
      "We usually train a neural network to predict these probabilities. A neural network trained on a \n",
      "large corpora of text is known as a Large Language Model (LLM).P [ “China” | “Shanghai is a city in” ]\n",
      "Prompt Next TokenUmar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesHow do we train and inference a Language Model?\n",
      "Training\n",
      "A language model is trained on a corpora of text, that is, a large collection of documents. Often, Language Models are \n",
      "trained on the entire Wikipedia and millions of web pages. This allows the Language Model to acquire as much knowledge \n",
      "as possible.\n",
      "We usually train a Transformer -based neural network as Language Model.\n",
      "Inference\n",
      "To inference a Language Model, we build a prompt and let the Language Model generate the rest by iteratively adding tokens.\n",
      "Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesYou are what you eat\n",
      "A language model can only output text and information that it was trained upon. This means, \n",
      "that if we train a language model only on English content, very probably it won’t be able to \n",
      "output Japanese or French. To teach new concepts, we need to fine -tune the model. Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesThe cons of fine -tuning\n",
      "•It can be expensive.\n",
      "•The number of parameters of the model may not be sufficient to capture all the knowledge \n",
      "we want to teach to it. That’s why LLaMA  was introduced with 7B, 13B and 70B parameters. \n",
      "•Fine -Tuning is not additive. It may replace existing knowledge of the model with new \n",
      "knowledge. For example, a language model trained on English that is (heavily) fine -tuned on \n",
      "Japanese may “forget” English.Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesPrompt Engineering to the rescue!\n",
      "It is possible to “teach ” a language model how to perform a new task by playing with the \n",
      "prompt. For example, by using “few-shot ” prompting. The following is an example:\n",
      "Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesQA with Prompt Engineering\n",
      "Context\n",
      "AnswerQuestionPromptInstructionsUmar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesThe pros of fine -tuning\n",
      "•Higher quality results compared to prompt engineering.\n",
      "•Smaller context size (input size) during inference since we don ’t need to include the context \n",
      "and instructions.Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "•Intro to Large Language Models\n",
      "•RAG pipeline\n",
      "•Embedding vectors\n",
      "•Sentence BERT\n",
      "•Vector DB\n",
      "•Algorithms (HNSW)Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesQA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "•Intro to Large Language Models\n",
      "•RAG pipeline\n",
      "•Embedding vectors\n",
      "•Sentence BERT\n",
      "•Vector DB\n",
      "•Algorithms (HNSW)Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesWhy do we use vectors to represent words?\n",
      "Given the words “ cherry ”, “digital ” and “ information ”, if we represent the embedding vectors \n",
      "using only 2 dimensions (X, Y) and we plot them, we hope to see something like this: the angle \n",
      "between words with similar meaning is small, while the angle between words with different \n",
      "meaning is big. So, the embeddings “capture” the meaning of the words they represent by \n",
      "projecting them into a high -dimensional space.\n",
      "Source: Speech and Language Processing 3rd Edition Draft, Dan Jurafsky  and James H. Martin\n",
      "We commonly use the cosine similarity , which is based on the dot product between the two \n",
      "vectors.Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesWord embeddings: the ideas\n",
      "•Words that are synonyms tend to occur in the same context (surrounded by the same \n",
      "words). \n",
      "•For example, the word “teacher ” and “professor ” usually occur surrounded by the \n",
      "words “school ”,  “university ”, “exam ”, “lecture ”, “course ”, etc..\n",
      "•The inverse can also be true: words that occur in the same context tend to have similar \n",
      "meanings. This is known as the distributional hypothesis .\n",
      "•This means that to capture the meaning of the word, we also need to have access to its \n",
      "context (the words surrounding it).\n",
      "•This is why we employ the Self -Attention mechanism in the Transformer model to capture \n",
      "contextual information for every token. The Self -Attention mechanism relates every token to \n",
      "all the other tokens in the sentence.Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesWord embeddings: the Cloze task\n",
      "•Imagine I give you the following sentence: \n",
      "Rome is the _______  of Italy, which is why it hosts many government buildings. \n",
      "Can you tell me what is the missing word?\n",
      "•Of course! The missing word is “capital ”, because by looking at the rest of the sentence, it is \n",
      "the one that makes the most sense.\n",
      "•This is how we train BERT: we want the Self -Attention mechanism to relate all the input \n",
      "tokens with each other, so that BERT has enough information about the “context ” of the \n",
      "missing word to predict it.Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesHow do we train embedding vectors in BERT?\n",
      "Input  (14 tokens):Output  (14 tokens):Target  (1 token): capital\n",
      "Run backpropagation  to update the weights Loss\n",
      "Rome is the [mask]  of Italy, which is why it hosts many government buildings.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13 TK14Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings\n",
      "•We can use the Self -Attention mechanism also to capture the “meaning” of an entire \n",
      "sentence.\n",
      "•We can use a pre -trained BERT model to produce embeddings of entire sentences. Let’s \n",
      "see howUmar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings with BERT\n",
      "Input  (13 tokens):Output  (13 tokens):\n",
      "Our professor always gives us lots of assignments to do in the weekend.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13\n",
      "Output of the Self -Attention : matrix of shape (13, 768)\n",
      "Input of the Self -Attention : matrix of shape (13, 768)Mean -Pooling: take the average of all the vectorsSentence EmbeddingSingle vector with 768 dimensionsUmar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings: comparison\n",
      "•How can we compare Sentence Embeddings to see if two sentences have similar \n",
      "“meaning ”? We could use the cosine similarity, which measures the cosine of the angle \n",
      "between the two vectors. A small angle results in a high cosine similarity score.\n",
      "•But there’s a problem : nobody told BERT that the embeddings it produces should be \n",
      "comparable with the cosine similarity, that is, two similar sentences should be represented \n",
      "by vectors pointing to the same direction in space. How can we teach BERT to produce \n",
      "embeddings that can be compared with a similarity function of our choice?Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "•Intro to Large Language Models\n",
      "•RAG pipeline\n",
      "•Embedding vectors\n",
      "•Sentence BERT\n",
      "•Vector DB\n",
      "•Algorithms (HNSW)Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesIntroducing Sentence BERT\n",
      "Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesSentence BERT: architecture\n",
      "Sentence A Sentence BBERTPoolingSentence Embedding A\n",
      "BERTPoolingSentence Embedding B\n",
      "Same architecture\n",
      "Same parameters\n",
      "Same weightsCosine SimilarityTarget  (Cosine Similarity)\n",
      "Loss (MSE)\n",
      "We can apply a linear layer to \n",
      "reduce the size of the \n",
      "embedding vector, for \n",
      "example from 768 to 512We can use mean -pooling, \n",
      "max -pooling or just use the [ cls] \n",
      "token as sentence embedding.Siamese Network\n",
      "“My father plays with my me at the park” “I play with my dad at the park ”Umar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesQA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesStrategies to teach new concepts to LLM\n",
      "Base LLM\n",
      "Fine -tune on custom data\n",
      "Fine -tuned LLM+\n",
      "=Base LLM\n",
      "Vector DB + Embeddings\n",
      "RAG+\n",
      "=\n",
      "Fine -tuned LLM + RAGOutline\n",
      "•Intro to Large Language Models\n",
      "•RAG pipeline\n",
      "•Embedding vectors\n",
      "•Sentence BERT\n",
      "•Vector DB\n",
      "•Algorithms (HNSW)Vector DB: introduction\n",
      "A vector database stores vectors of fixed dimensions (called embeddings) such that we can then query the \n",
      "database to find all the embeddings that are closest (most similar) to a given query vector using a distance \n",
      "metric, which is usually the cosine similarity, but we can also use the Euclidean distance. The database uses \n",
      "a variant of the KNN (K Nearest Neighbor) algorithm or another similarity search algorithm. Vector DBs are \n",
      "also used for finding similar songs (e.g. Spotify), images (e.g. Google Images) or products (e.g. Amazon).\n",
      "Query\n",
      "Vector DBWeb Pages DocumentsHow many parameters are there in Grok -0?\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with 33 billion parameters. This early model approaches \n",
      "LLaMA  2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources. \n",
      "[...]Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KK-NN: a naïve approach\n",
      "Imagine we want to search for the query in our database: a simple way would be comparing \n",
      "the query with all the vectors, sorting them by distance, and keeping the top K.\n",
      "QueryHow many parameters are there in Grok -0?Emb . 1\n",
      "Emb . 2\n",
      "Emb . 3\n",
      "…\n",
      "Emb . 1,000,000…\n",
      "…\n",
      "…\n",
      "…\n",
      "Emb . 999,998\n",
      "Emb . 999,999Distance Vector\n",
      "0.3\n",
      "0.1\n",
      "0.2\n",
      "0.9\n",
      "0.2\n",
      "0.70.4\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.6Keep top KIf there are N embedding vectors and each \n",
      "has D dimensions, the computational \n",
      "complexity is in the order of O(N*D), too \n",
      "slow!🐌Outline\n",
      "•Intro to Large Language Models\n",
      "•RAG pipeline\n",
      "•Embedding vectors\n",
      "•Sentence BERT\n",
      "•Vector DB\n",
      "•Algorithms (HNSW)Similarity Search: let’s trade precision for speed\n",
      "The naïve approach we used before, always produces accurate \n",
      "results, since it compares the query with all the stored vectors, but \n",
      "what if we reduced the number of comparison, but still obtain \n",
      "accurate results with high probability?\n",
      "The metric we usually care about in Similarity Search is recall.\n",
      "In this video we will explore an algorithm for Approximate Nearest \n",
      "Neighbors, called Hierarchical Navigable Small Worlds (HNSW) . \n",
      "Source: WikipediaHNSW in the real world\n",
      "It is the same algorithm that powers Qdrant , the open source Vector DB used by Twitter’s  (X) Grok  LLM, \n",
      "which can access tweets in real time.\n",
      "HNSW: idea # 1\n",
      "HNSW is an evolution of the Navigable Small Worlds  algorithm for Approximate Nearest Neighbors, which \n",
      "is based on the concept of Six Degrees of Separation .\n",
      "Milgram's experiment aimed to test the social connections among people in the United States. The \n",
      "participants, who were initially located in Nebraska and Kansas, were given a letter to be delivered to a \n",
      "specific person in Boston. However, they were not allowed to send the letter directly to the recipient. \n",
      "Instead, they were instructed to send it to someone they knew on a first -name basis, who they believed \n",
      "might have a better chance of knowing the target person.\n",
      "At the end of Milgram ’s small -world experiment, Milgram found that most of the letters reached the final \n",
      "recipient in five or six steps, creating the concept that people all over the world are all connected by six \n",
      "degrees of separation.\n",
      "Facebook found in 2016 that its 1.59 billion active users were connected on average by 3.5 degrees of \n",
      "separation: https://research.facebook.com/blog/ 2016 /02/three -and-a-half-degrees -of-separation/\n",
      "This means that you and Mark Zuckerberg are only 3.5 connections apart!Navigable Small Worlds\n",
      "The NSW algorithm builds a graph that – just like Facebook friends – connects close vectors with each other \n",
      "but keeping the total number of connections small. For example, every vector may be connected to up to 6 \n",
      "other vectors (to mimic the Six Degrees of Separation).\n",
      "0102\n",
      "0305\n",
      "04\n",
      "0609\n",
      "0813\n",
      "11\n",
      "121510 07\n",
      "14Node Text\n",
      "01 […] The Transformer is a model […]\n",
      "02 […] Diagnose cancer with AI […]\n",
      "03 […] A transformer -based model […]\n",
      "04 […] The Transformer has 6 layers […]\n",
      "05 […] An MRI machine that costs 1$ […]\n",
      "06 […] The dot -product is a […]\n",
      "07 […] Big -Pharma is not so big […]\n",
      "08 […] Cross -Attention is a great […]\n",
      "09 […] To solve an ODE […]\n",
      "10 […] We are aging too fast […]\n",
      "11 […] Open -source models like […]\n",
      "12 […] MathBERT : a new model […]\n",
      "13 […] AI to control aging […]\n",
      "14 […] Attention is all you need […]\n",
      "15 […] LLaMA  2 has 7B params […]Navigable Small Worlds: searching for K -NN\n",
      "Given the following query: “ How many Encoder layers are there in the Transformer model? ”\n",
      "How does the algorithm find the K Nearest Neighbors?\n",
      "0102\n",
      "0305\n",
      "04\n",
      "0609\n",
      "0813\n",
      "11\n",
      "121510 07\n",
      "14Node Text\n",
      "01 […] The Transformer is a model […]\n",
      "02 […] Diagnose cancer with AI […]\n",
      "03 […] A transformer -based model […]\n",
      "04 […] The Transformer has 6 layers […]\n",
      "05 […] An MRI machine that costs 1$ […]\n",
      "06 […] The dot -product is a […]\n",
      "07 […] Big -Pharma is not so big […]\n",
      "08 […] Cross -Attention is a great […]\n",
      "09 […] To solve an ODE […]\n",
      "10 […] We are aging too fast […]\n",
      "11 […] Open -source models like […]\n",
      "12 […] MathBERT : a new model […]\n",
      "13 […] AI to control aging […]\n",
      "14 […] Attention is all you need […]\n",
      "15 […] LLaMA  2 has 7B params […]Q\n",
      "Local best: stop search\n",
      "We repeat the search with randomly chosen starting \n",
      "points and then keep the top K among all the visited \n",
      "nodes.Navigable Small Worlds: inserting a new vector\n",
      "We can insert a new vector by searching the top KNN with the searching algorithm described before and \n",
      "making an edge between the vector and the top K results.HNSW: idea #2\n",
      "To go from NSW (Navigable Small Worlds) to HNSW (Hierarchical Navigable Small Worlds), we need to \n",
      "introduce the algorithm behind the data structure known as Skip -List.\n",
      "The skip list is a data structure that maintains a sorted list and allows search and insertion with an average of \n",
      "𝑂(log𝑁) time complexity.\n",
      "H3\n",
      "H2\n",
      "H1\n",
      "H0 EEEE\n",
      "1 5 3 9 12 17 23Let’s search the number 9HNSW: Hierarchical Navigable Small Worlds\n",
      "HNSW: Hierarchical Navigable Small Worlds\n",
      "Q\n",
      "Layer 0 (dense)Layer 3 (sparse)\n",
      "Layer 1Layer 2Let’s search!\n",
      "We repeat the search with \n",
      "randomly chosen starting \n",
      "points (on the top layer) and \n",
      "then keep the top K among \n",
      "all the visited nodes.QA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil –https://github.com/hkproj/retrieval -augmented -generation -notesThanks for watching!\n",
      "Don’t forget to subscribe for \n",
      "more amazing content on AI \n",
      "and Machine Learning!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        # Initialize a variable to store the extracted text\n",
    "        extracted_text = \"\"\n",
    "        \n",
    "        # Iterate through all the pages and extract text\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            extracted_text += page.extract_text()\n",
    "        \n",
    "        return extracted_text\n",
    "\n",
    "# Example usage\n",
    "pdf_file = 'RAG.pdf'  # Specify your PDF file path here\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Retrieval Augmented Generation (RAG)\n",
      "\n",
      "## Index\n",
      "\n",
      "### 1. Introduction\n",
      "   - What is a Language Model?\n",
      "   - How do we train and inference a Language Model?\n",
      "   - The cons of fine-tuning\n",
      "   - Prompt Engineering\n",
      "   - QA with Prompt Engineering\n",
      "   - The pros of fine-tuning\n",
      "   - QA with Retrieval Augmented Generation\n",
      "\n",
      "### 2. Embedding Vectors\n",
      "   - Why do we use vectors to represent words?\n",
      "   - Word embeddings: the ideas\n",
      "   - Word embeddings: the Cloze task\n",
      "   - How do we train embedding vectors in BERT?\n",
      "   - Sentence Embeddings\n",
      "   - Sentence Embeddings with BERT\n",
      "   - Sentence Embeddings: comparison\n",
      "   - Introducing Sentence BERT\n",
      "   - Sentence BERT: architecture\n",
      "   - Strategies to teach new concepts to LLM\n",
      "\n",
      "### 3. Vector DB\n",
      "   - Vector DB: introduction\n",
      "   - K-NN: a naïve approach\n",
      "   - Similarity Search: let's trade precision for speed\n",
      "   - HNSW in the real world\n",
      "   - HNSW: idea #1\n",
      "   - Navigable Small Worlds\n",
      "   - Navigable Small Worlds: searching for K-NN\n",
      "   - Navigable Small Worlds: inserting a new vector\n",
      "   - HNSW: idea #2\n",
      "   - HNSW: Hierarchical Navigable Small Worlds\n",
      "   - HNSW: Hierarchical Navigable Small Worlds\n",
      "\n",
      "## Retrieval Augmented Generation (RAG)\n",
      "\n",
      "### Intro to Large Language Models\n",
      "\n",
      "A language model is a probabilistic model that assigns probabilities to sequences of words. \n",
      "\n",
      "In practice, a language model allows us to compute the following: \n",
      "\n",
      "```\n",
      "P [ \"China\" | \"Shanghai is a city in\" ]\n",
      "```\n",
      "\n",
      "We usually train a neural network to predict these probabilities. A neural network trained on a large corpus of text is known as a Large Language Model (LLM). \n",
      "\n",
      "### RAG Pipeline\n",
      "\n",
      "The RAG pipeline consists of the following steps:\n",
      "\n",
      "1. **Split into chunks:** The input document is split into smaller chunks.\n",
      "2. **Embeddings:** Each chunk is converted into an embedding vector using Sentence BERT.\n",
      "3. **Store:** The embedding vectors are stored in a vector database.\n",
      "4. **Search:** When a query is given, its embedding vector is calculated and used to search the vector database for the most similar chunks.\n",
      "5. **Top-K:** The top-K most similar chunks are retrieved from the vector database.\n",
      "6. **LLM:** The retrieved chunks are fed into a large language model (LLM) along with the query.\n",
      "7. **Prompt:** The LLM generates a response based on the query and the retrieved context.\n",
      "\n",
      "### Embedding Vectors\n",
      "\n",
      "#### Why do we use vectors to represent words?\n",
      "\n",
      "Given the words \"cherry\", \"digital\" and \"information\", if we represent the embedding vectors using only 2 dimensions (X, Y) and we plot them, we hope to see something like this: the angle between words with similar meaning is small, while the angle between words with different meaning is big. So, the embeddings \"capture\" the meaning of the words they represent by projecting them into a high-dimensional space. \n",
      "\n",
      "We commonly use the cosine similarity, which is based on the dot product between the two vectors.\n",
      "\n",
      "#### Word embeddings: the ideas\n",
      "\n",
      "- Words that are synonyms tend to occur in the same context (surrounded by the same words). \n",
      "- For example, the word \"teacher\" and \"professor\" usually occur surrounded by the words \"school\", \"university\", \"exam\", \"lecture\", \"course\", etc..\n",
      "- The inverse can also be true: words that occur in the same context tend to have similar meanings. This is known as the **distributional hypothesis**.\n",
      "- This means that to capture the meaning of the word, we also need to have access to its context (the words surrounding it).\n",
      "- This is why we employ the Self-Attention mechanism in the Transformer model to capture contextual information for every token. The Self-Attention mechanism relates every token to all the other tokens in the sentence.\n",
      "\n",
      "#### Word embeddings: the Cloze task\n",
      "\n",
      "- Imagine I give you the following sentence: \n",
      "  Rome is the _______  of Italy, which is why it hosts many government buildings. \n",
      "  Can you tell me what is the missing word?\n",
      "- Of course! The missing word is \"capital\", because by looking at the rest of the sentence, it is the one that makes the most sense.\n",
      "- This is how we train BERT: we want the Self-Attention mechanism to relate all the input tokens with each other, so that BERT has enough information about the \"context\" of the missing word to predict it.\n",
      "\n",
      "#### How do we train embedding vectors in BERT?\n",
      "\n",
      "```\n",
      "Input  (14 tokens):  Output  (14 tokens):  Target  (1 token): capital\n",
      "Rome is the [mask]  of Italy, which is why it hosts many government buildings.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13 TK14\n",
      "```\n",
      "\n",
      "We run backpropagation to update the weights and minimize the loss.\n",
      "\n",
      "#### Sentence Embeddings\n",
      "\n",
      "We can use the Self-Attention mechanism also to capture the \"meaning\" of an entire sentence. We can use a pre-trained BERT model to produce embeddings of entire sentences. \n",
      "\n",
      "#### Sentence Embeddings with BERT\n",
      "\n",
      "```\n",
      "Input  (13 tokens):  Output  (13 tokens):\n",
      "Our professor always gives us lots of assignments to do in the weekend.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13\n",
      "```\n",
      "\n",
      "The output of the Self-Attention is a matrix of shape (13, 768). We can then apply mean-pooling to take the average of all the vectors, resulting in a single vector with 768 dimensions - our sentence embedding.\n",
      "\n",
      "#### Sentence Embeddings: comparison\n",
      "\n",
      "How can we compare Sentence Embeddings to see if two sentences have similar \"meaning\"? We could use the cosine similarity, which measures the cosine of the angle between the two vectors. A small angle results in a high cosine similarity score. \n",
      "\n",
      "But there's a problem: nobody told BERT that the embeddings it produces should be comparable with the cosine similarity, that is, two similar sentences should be represented by vectors pointing to the same direction in space. How can we teach BERT to produce embeddings that can be compared with a similarity function of our choice? \n",
      "\n",
      "### Introducing Sentence BERT\n",
      "\n",
      "Sentence BERT is a variation of BERT that is specifically trained to produce sentence embeddings that are comparable using cosine similarity. It does this by using a Siamese network.\n",
      "\n",
      "#### Sentence BERT: architecture\n",
      "\n",
      "Sentence BERT uses two identical BERT models, one for each input sentence. The outputs of the BERT models are then fed into a pooling layer to produce sentence embeddings. The sentence embeddings are then compared using cosine similarity, and the loss is calculated based on the difference between the cosine similarity and the target similarity score.\n",
      "\n",
      "```\n",
      "Sentence A\n",
      "BERT\n",
      "Pooling\n",
      "Sentence Embedding A\n",
      "Sentence B\n",
      "BERT\n",
      "Pooling\n",
      "Sentence Embedding B\n",
      "Cosine Similarity\n",
      "Target  (Cosine Similarity)\n",
      "Loss (MSE)\n",
      "```\n",
      "\n",
      "We can apply a linear layer to reduce the size of the embedding vector, for example from 768 to 512. We can use mean-pooling, max-pooling or just use the [cls] token as sentence embedding.\n",
      "\n",
      "#### Strategies to teach new concepts to LLM\n",
      "\n",
      "There are two main strategies for teaching new concepts to an LLM:\n",
      "\n",
      "1. **Fine-tuning:** The LLM is fine-tuned on a custom dataset that contains the desired knowledge. This can result in higher quality results compared to prompt engineering, but it can be expensive and may not be additive.\n",
      "2. **RAG:** The LLM is combined with a vector database that contains relevant context. This allows the LLM to access information from a wider range of sources, but it may not be as effective as fine-tuning for specific tasks.\n",
      "\n",
      "### Vector DB\n",
      "\n",
      "#### Vector DB: introduction\n",
      "\n",
      "A vector database stores vectors of fixed dimensions (called embeddings) such that we can then query the database to find all the embeddings that are closest (most similar) to a given query vector using a distance metric, which is usually the cosine similarity, but we can also use the Euclidean distance. The database uses a variant of the KNN (K Nearest Neighbor) algorithm or another similarity search algorithm. Vector DBs are also used for finding similar songs (e.g. Spotify), images (e.g. Google Images) or products (e.g. Amazon).\n",
      "\n",
      "#### K-NN: a naïve approach\n",
      "\n",
      "Imagine we want to search for the query in our database: a simple way would be comparing the query with all the vectors, sorting them by distance, and keeping the top K.\n",
      "\n",
      "If there are N embedding vectors and each has D dimensions, the computational complexity is in the order of O(N*D), too slow!🐌\n",
      "\n",
      "#### Similarity Search: let's trade precision for speed\n",
      "\n",
      "The naïve approach we used before, always produces accurate results, since it compares the query with all the stored vectors, but what if we reduced the number of comparison, but still obtain accurate results with high probability?\n",
      "\n",
      "The metric we usually care about in Similarity Search is recall.\n",
      "\n",
      "In this video we will explore an algorithm for Approximate Nearest Neighbors, called Hierarchical Navigable Small Worlds (HNSW). \n",
      "\n",
      "#### HNSW in the real world\n",
      "\n",
      "It is the same algorithm that powers Qdrant, the open source Vector DB used by Twitter’s (X) Grok LLM, which can access tweets in real time.\n",
      "\n",
      "#### HNSW: idea #1\n",
      "\n",
      "HNSW is an evolution of the Navigable Small Worlds algorithm for Approximate Nearest Neighbors, which is based on the concept of **Six Degrees of Separation**.\n",
      "\n",
      "Milgram's experiment aimed to test the social connections among people in the United States. The participants, who were initially located in Nebraska and Kansas, were given a letter to be delivered to a specific person in Boston. However, they were not allowed to send the letter directly to the recipient. Instead, they were instructed to send it to someone they knew on a first-name basis, who they believed might have a better chance of knowing the target person.\n",
      "\n",
      "At the end of Milgram’s small-world experiment, Milgram found that most of the letters reached the final recipient in five or six steps, creating the concept that people all over the world are all connected by six degrees of separation.\n",
      "\n",
      "Facebook found in 2016 that its 1.59 billion active users were connected on average by 3.5 degrees of separation: https://research.facebook.com/blog/2016/02/three-and-a-half-degrees-of-separation/\n",
      "\n",
      "This means that you and Mark Zuckerberg are only 3.5 connections apart!\n",
      "\n",
      "#### Navigable Small Worlds\n",
      "\n",
      "The NSW algorithm builds a graph that – just like Facebook friends – connects close vectors with each other but keeping the total number of connections small. For example, every vector may be connected to up to 6 other vectors (to mimic the Six Degrees of Separation).\n",
      "\n",
      "```\n",
      "01  02\n",
      "     \\\n",
      "      03\n",
      "     / \\\n",
      "    05  04\n",
      "   /  / \\\n",
      "  06  09  08\n",
      "       \\ /\n",
      "        13\n",
      "       / \\\n",
      "      11  12\n",
      "     /  \\  /\n",
      "    15  10 07\n",
      "        \\ /\n",
      "         14\n",
      "```\n",
      "\n",
      "#### Navigable Small Worlds: searching for K-NN\n",
      "\n",
      "Given the following query: “How many Encoder layers are there in the Transformer model?”\n",
      "\n",
      "How does the algorithm find the K Nearest Neighbors?\n",
      "\n",
      "```\n",
      "01  02\n",
      "     \\\n",
      "      03\n",
      "     / \\\n",
      "    05  04\n",
      "   /  / \\\n",
      "  06  09  08\n",
      "       \\ /\n",
      "        13\n",
      "       / \\\n",
      "      11  12\n",
      "     /  \\  /\n",
      "    15  10 07\n",
      "        \\ /\n",
      "         14\n",
      "\n",
      "Node Text\n",
      "01 […] The Transformer is a model […]\n",
      "02 […] Diagnose cancer with AI […]\n",
      "03 […] A transformer-based model […]\n",
      "04 […] The Transformer has 6 layers […]\n",
      "05 […] An MRI machine that costs 1$ […]\n",
      "06 […] The dot-product is a […]\n",
      "07 […] Big-Pharma is not so big […]\n",
      "08 […] Cross-Attention is a great […]\n",
      "09 […] To solve an ODE […]\n",
      "10 […] We are aging too fast […]\n",
      "11 […] Open-source models like […]\n",
      "12 […] MathBERT : a new model […]\n",
      "13 […] AI to control aging […]\n",
      "14 […] Attention is all you need […]\n",
      "15 […] LLaMA 2 has 7B params […]\n",
      "```\n",
      "\n",
      "We start from a randomly chosen node and then we keep moving to the closest neighbor. \n",
      "\n",
      "#### Navigable Small Worlds: inserting a new vector\n",
      "\n",
      "We can insert a new vector by searching the top KNN with the searching algorithm described before and making an edge between the vector and the top K results.\n",
      "\n",
      "#### HNSW: idea #2\n",
      "\n",
      "To go from NSW (Navigable Small Worlds) to HNSW (Hierarchical Navigable Small Worlds), we need to introduce the algorithm behind the data structure known as Skip-List.\n",
      "\n",
      "The skip list is a data structure that maintains a sorted list and allows search and insertion with an average of 𝑂(log𝑁) time complexity.\n",
      "\n",
      "```\n",
      "H3\n",
      "H2\n",
      "H1\n",
      "H0 EEEE\n",
      "1 5 3 9 12 17 23\n",
      "```\n",
      "\n",
      "Let’s search the number 9\n",
      "\n",
      "#### HNSW: Hierarchical Navigable Small Worlds\n",
      "\n",
      "HNSW uses a hierarchical structure of graphs, where each level is sparser than the previous one.\n",
      "\n",
      "```\n",
      "Q\n",
      "Layer 0 (dense)\n",
      "Layer 1\n",
      "Layer 2\n",
      "Layer 3 (sparse)\n",
      "```\n",
      "\n",
      "Let’s search!\n",
      "\n",
      "We repeat the search with randomly chosen starting points (on the top layer) and then keep the top K among all the visited nodes.\n",
      "\n",
      "## Thanks for watching!\n",
      "\n",
      "Don’t forget to subscribe for more amazing content on AI and Machine Learning!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''Your task is to covert the following text into \n",
    "markdown format with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "\n",
    "Also before this first write index in markdown as well with appropriate headings and subheadings.\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Run_pdf.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_string_to_markdown(response.text, 'Run_pdf.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction The purpose of this book is t0 describe the optical structur and Oplical properties of the humnan eye_ It Will be useful [0 those who have an inleTeS[ in Vision. such as optometrists, ophthalmologists. vision scientists, Optical physicists. and sludents 0l visual optics.An understanding of the optics of the humaneye is panticularly important designers of ophthalmic diagnostic equipment and visual optical systems. such as telescopes_ Many animals have some SOrt of eye stnucture or sophisticated light sense Like humans. some rely heavily on vision. including predatory birds and insects such as honeybees and dragonflies- However; many animals rely much more on Other senses. particularly hearing and smell . than on vision_ The Visual sense iS Very complex and can process amounts of information Very rapidly How this is done is nOt fully understood: requires greater knowledge of how the neural components of vision (relina_ visual cortex, and other brain centers) process the retinal image. However. the first stage in this complex process is the fonation of the retinal image. In this [ext we investigale how the image is forined and discuss factors that affect its qualily- Most animal eyes can be divided into IWO groups: compound eyes (aS possessed by most insects) and verlebrale eyes (such as the human eye). Compared With Ver- tebrate eyes. there is considerable variation in the compound eyes. Compound eyes cOntain many optical elements (ommatidia). each with own aperture [0 the extemal world  Vertebrate eyes have single aperure [0 the extemal world. which is used by all the detectors. Several other animals have simple eyes which can be described developed versions of the vertebrate eye: All eyes- of whatever type . involve compromises between the need for detection (senSiIvily ) - panticularly at Iow light levels . and resolving capability in terms of the direction or formn of an object Although this book is about the of the human €ye. we do not wish t0 con- sider the optics in complete isolation from the neural components. othenwvise We canol appreciate what influence changes in the retinal image will have on vision pertormance. As an example. altering the optics considerable influence on Teso- lution of objects for central vision but not for peripheral vision. This because the retina neural structure 1S fine enough at ItS center but not in the penphery- fOr large changes in optical quality to be of imponance (Chapter 18). Thus. the neural components of the visual system particularly the retinal detector. rate some mention in the book: The neural structures of the retina themselves produce optical effects-As an examnple the photoreceptors exhibit waveguide properties that make light ariving from directions more efficient at stimulating vision than light arriving from other directions Another example is that the regular arrangement of the nerve fiber layers produces polanization etiects. While image formnation in the eye is simnilar t0 that in man-made optical systems. such as cumeras and muSt theconventional optical Iaws thete   are   some interesting differences because of the eye biological basis. Perhaps the greatest diffetence iS that. as living organ. the eye responds t0 its environment. often t0 give the best image under different circumstances: Also. grows. suffers huge spatial oplics ha? SOle obey and AEC\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "def detect_text_with_easyocr(image_path):\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    results = reader.readtext(image_path)\n",
    "    \n",
    "    detected_text = ' '.join([result[1] for result in results])\n",
    "    return detected_text\n",
    "\n",
    "image_path = 'temp.png'\n",
    "text = detect_text_with_easyocr(image_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Index\n",
      "\n",
      "### I. Introduction\n",
      "\n",
      "#### 1.1. Purpose of the Book\n",
      "\n",
      "#### 1.2. Importance of Understanding Human Eye Optics\n",
      "\n",
      "#### 1.3. The Visual Sense in Different Animals\n",
      "\n",
      "#### 1.4. Image Formation and Its Quality\n",
      "\n",
      "#### 1.5. Types of Animal Eyes\n",
      "\n",
      "#### 1.6. Compromises in Eye Design\n",
      "\n",
      "#### 1.7. Importance of Neural Components\n",
      "\n",
      "#### 1.8. Optical Effects Produced by Retinal Structures\n",
      "\n",
      "#### 1.9. Differences between Human and Artificial Optical Systems\n",
      "\n",
      "---\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The purpose of this book is to describe the optical structure and optical properties of the human eye. It will be useful to those who have an interest in vision, such as optometrists, ophthalmologists, vision scientists, optical physicists, and students of visual optics. An understanding of the optics of the human eye is particularly important for designers of ophthalmic diagnostic equipment and visual optical systems, such as telescopes. \n",
      "\n",
      "Many animals have some sort of eye structure or sophisticated light sense. Like humans, some rely heavily on vision, including predatory birds and insects such as honeybees and dragonflies. However, many animals rely much more on other senses, particularly hearing and smell, than on vision. \n",
      "\n",
      "The visual sense is very complex and can process vast amounts of information very rapidly. How this is done is not fully understood; it requires greater knowledge of how the neural components of vision (retina, visual cortex, and other brain centers) process the retinal image. However, the first stage in this complex process is the formation of the retinal image. In this text, we investigate how the image is formed and discuss factors that affect its quality. \n",
      "\n",
      "Most animal eyes can be divided into two groups: compound eyes (as possessed by most insects) and vertebrate eyes (such as the human eye). Compared with vertebrate eyes, there is considerable variation in the compound eyes. Compound eyes contain many optical elements (ommatidia), each with its own aperture to the external world. Vertebrate eyes have a single aperture to the external world, which is used by all the detectors. Several other animals have simple eyes which can be described as developed versions of the vertebrate eye.\n",
      "\n",
      "All eyes, of whatever type, involve compromises between the need for detection (sensitivity), particularly at low light levels, and resolving capability in terms of the direction or form of an object. \n",
      "\n",
      "Although this book is about the optics of the human eye, we do not wish to consider the optics in complete isolation from the neural components. Otherwise, we cannot appreciate what influence changes in the retinal image will have on vision performance. As an example, altering the optics has considerable influence on the resolution of objects for central vision but not for peripheral vision. This is because the retina's neural structure is fine enough at its center but not in the periphery for large changes in optical quality to be of importance (Chapter 18). Thus, the neural components of the visual system, particularly the retinal detector, rate some mention in the book.\n",
      "\n",
      "The neural structures of the retina themselves produce optical effects. As an example, the photoreceptors exhibit waveguide properties that make light arriving from certain directions more efficient at stimulating vision than light arriving from other directions. Another example is that the regular arrangement of the nerve fiber layers produces polarization effects. \n",
      "\n",
      "While image formation in the eye is similar to that in man-made optical systems, such as cameras, and must obey the conventional optical laws, there are some interesting differences because of the eye's biological basis. Perhaps the greatest difference is that, as a living organ, the eye responds to its environment, often to give the best image under different circumstances. Also, it grows, suffers huge spatial optics, and has some ability to adapt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Run_image.md\n"
     ]
    }
   ],
   "source": [
    "save_string_to_markdown(response.text, 'Run_image.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\documentclass{article}\n",
      "\\title{Workflow for VR-Phore}\n",
      "\\author{}\n",
      "\\date{}\n",
      "\\begin{document}\n",
      "\\maketitle\n",
      "\n",
      "\\section{Pre-tasks}\n",
      "\n",
      "Download the APKs onto smartphones (image resolution improves with high-quality devices; ensure the Display Mobile (DM) fits securely into the VRHMD slot). Prepare the controller and application setup.\n",
      "\n",
      "\\section{Clinical Tasks}\n",
      "\n",
      "\\subsection{By Clinician}\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item Select the number of slides for each test (SMP, Fusion, and Stereopsis).\n",
      "\\item Choose the number of macular or peripheral test slides.\n",
      "\\item (Version 2): A set of slides can be shown for clinicians to select for each test.\n",
      "\\item Measure IPD:\n",
      "    \\begin{itemize}\n",
      "        \\item Method 1: Use standard clinical methods (e.g., scale).\n",
      "        \\item Method 2: Adjust the lens in the VR HMD (not usually optimal if directly done by a strabismus patient).\n",
      "        \\item A rough way to measure IPD would be to set the patient to look straight and check if the images are centered (similar to a synoptophore). Then move the slides equally apart or close together as in convergence/divergence sections.\n",
      "    \\end{itemize}\n",
      "\\item Calibration -- Center the images on each eye's display. Usually done by a person without deviation or phoria, who wears the HMD to check if the default-centered images are fused.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Patient Instructions}\n",
      "\n",
      "The patient is given instructions to:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item Operate the controller -- use the scroll action to move images and double-click anywhere on the screen to indicate alignment completion.\n",
      "\\item Move images using one (single image) or two thumbs (both images simultaneously).\n",
      "\\item Perceive fusion, for example, recognizing a lion inside a cage or seeing a complete mouse or cat.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Fitting the VRHMD}\n",
      "\n",
      "The VRHMD is placed gently on the patient's head while they are seated, with the straps tightened if needed. A few sample slides are shown, and the child is asked:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item Do you see images?\n",
      "\\item Are they near the center or away?\n",
      "\\item Close one eye and describe the image (repeat with the other eye).\n",
      "\\item Is the display too bright?\n",
      "\\item Try using the controllers.\n",
      "\\end{itemize}\n",
      "\n",
      "The clinician enters the patient's details on the controller (or possibly the display mobile). The clinician can select whether the left or right eye image is fixed, or if both images will move. The SMP/Fusion and Stereopsis sets are shown in a predefined or selectable order (Version 2 of the application allows the clinician to choose the order). Position and angle data from the calibrated center are logged, with a 6-second gap between slides.\n",
      "\n",
      "\\subsection{Additional Control Options}\n",
      "\n",
      "The clinician can select the axis of movement, such as moving linearly (one-axis) or in a plane (two-axis). The patient can rotate the image perpendicularly on one axis, enabling measurement of torsional values. For example, swiping left or right moves the image linearly, while upward and downward swipes rotate it clockwise and counterclockwise, respectively. Convergence ranges can also be measured, including degrees to which the participant can achieve fusion.\n",
      "\n",
      "\\section{Post-Task}\n",
      "\n",
      "Once the deck is complete, the HMD and controller are gently removed from the patient. The clinician can check a CSV file, which displays the angle values for each slide set/test.\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "\n",
    "Workflow for VR-Phore\n",
    "\n",
    "Pre-tasks: Download the APKs onto smartphones (image resolution improves with high-quality devices; ensure the Display Mobile (DM) fits securely into the VRHMD slot). Prepare the controller and application setup.\n",
    "\n",
    "Clinical Tasks\n",
    "\n",
    "By clinician:\n",
    "\n",
    "Select the number of slides for each test (SMP, Fusion, and Stereopsis).\n",
    "Choose the number of macular or peripheral test slides.\n",
    "(Version 2): A set of slides can be shown for clinicians to select for each test.\n",
    "Measure IPD:\n",
    "\n",
    "Method 1: Use standard clinical methods (e.g., scale).\n",
    "Method 2: Adjust the lens in the VR HMD (not usually optimal if directly done by a strabismus patient).\n",
    "A rough way to measure IPD would be to set the patient to look straight and check if the images are centered (similar to a synoptophore). Then move the slides equally apart or close together as in convergence/divergence sections.\n",
    "Calibration – Center the images on each eye's display. Usually done by a person without deviation or phoria, who wears the HMD to check if the default-centered images are fused.\n",
    "\n",
    "Patient instructions: The patient is given instructions to:\n",
    "\n",
    "Operate the controller – use the scroll action to move images and double-click anywhere on the screen to indicate alignment completion.\n",
    "Move images using one (single image) or two thumbs (both images simultaneously).\n",
    "Perceive fusion, for example, recognizing a lion inside a cage or seeing a complete mouse or cat.\n",
    "Fitting the VRHMD: The VRHMD is placed gently on the patient's head while they are seated, with the straps tightened if needed. A few sample slides are shown, and the child is asked:\n",
    "\n",
    "Do you see images?\n",
    "Are they near the center or away?\n",
    "Close one eye and describe the image (repeat with the other eye).\n",
    "Is the display too bright?\n",
    "Try using the controllers.\n",
    "The clinician enters the patient's details on the controller (or possibly the display mobile). The clinician can select whether the left or right eye image is fixed, or if both images will move. The SMP/Fusion and Stereopsis sets are shown in a predefined or selectable order (Version 2 of the application allows the clinician to choose the order). Position and angle data from the calibrated center are logged, with a 6-second gap between slides.\n",
    "\n",
    "Additional Control Options: The clinician can select the axis of movement, such as moving linearly (one-axis) or in a plane (two-axis). The patient can rotate the image perpendicularly on one axis, enabling measurement of torsional values. For example, swiping left or right moves the image linearly, while upward and downward swipes rotate it clockwise and counterclockwise, respectively. Convergence ranges can also be measured, including degrees to which the participant can achieve fusion.\n",
    "\n",
    "Post-Task: Once the deck is complete, the HMD and controller are gently removed from the patient. The clinician can check a CSV file, which displays the angle values for each slide set/test.\n",
    "\n",
    "'''\n",
    "\n",
    "query = r'''\n",
    "Your task is to covert the following text into simple Latex code with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "Keep in mind to account for speacial characters such as:\n",
    "# should be \\#\n",
    "$ should be \\$\n",
    "% should be \\%\n",
    "& should be \\&\n",
    "_ should be \\_\n",
    "\n",
    "and so on.\n",
    "\n",
    "\n",
    "\n",
    "Make sure to use only ascii characters for the latex code.\n",
    "\n",
    "Also just directly give the latex code, nothing else should be included in output.\n",
    "This is the template you can use:\n",
    "'''\n",
    "\n",
    "template = r'''\n",
    "Use the following template:\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage{amsmath}   % For mathematical formulas\n",
    "\\usepackage{graphicx}  % For including graphics\n",
    "\\usepackage{cite}      % For bibliography management\n",
    "\\usepackage{hyperref}  % For hyperlinks\n",
    "\n",
    "\\title{Title of the Research Paper}\n",
    "\\author{\\IEEEauthorblockN{Your Name}\n",
    "\\IEEEauthorblockA{Your Institution\\\\\n",
    "Your Email}\n",
    "}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\begin{abstract}\n",
    "    This is a brief summary of the research paper, providing an overview of the main objectives, methodology, results, and conclusions. The abstract should be concise, usually around 150-250 words.\n",
    "\\end{abstract}\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "    Keyword1, Keyword2, Keyword3, Keyword4\n",
    "\\end{IEEEkeywords}\n",
    "\n",
    "\\section{Introduction}\n",
    "The introduction provides background information on the topic of the research, outlines the problem being addressed, and states the objectives of the study. It may also include a brief literature review.\n",
    "\n",
    "\\section{Methodology}\n",
    "In this section, describe the methods used in your research. Include information about the experimental design, materials, procedures, and any statistical analyses performed.\n",
    "\n",
    "\\section{Results}\n",
    "Present the findings of your research. Use tables, figures, and graphs where necessary to illustrate your results. Each table or figure should be accompanied by a caption explaining its contents.\n",
    "\n",
    "\\subsection{Subsection Title}\n",
    "If needed, you can include subsections to further organize your results.\n",
    "\n",
    "\\section{Discussion}\n",
    "Interpret the results in this section. Discuss their implications, limitations, and how they relate to previous research. Consider addressing any unexpected findings.\n",
    "\n",
    "\\section{Conclusion}\n",
    "Summarize the main findings and contributions of your research. You may also suggest directions for future research.\n",
    "\n",
    "\\section*{Acknowledgments}\n",
    "Acknowledge any individuals or organizations that contributed to your research but do not meet the criteria for authorship.\n",
    "\n",
    "\\bibliographystyle{IEEEtran}  % IEEE bibliography style\n",
    "\\bibliography{references}       % Include your bibliography file (references.bib)\n",
    "\n",
    "\\end{document}\n",
    "\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "response = model.generate_content(query + template + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def string_to_tex_file(latex_string, filename):\n",
    "    \"\"\"Convert a LaTeX string to a .tex file.\"\"\"\n",
    "\n",
    "    with open(filename, 'w') as tex_file:\n",
    "        tex_file.write(latex_string)\n",
    "    print(f\"Written LaTeX content to {filename}\")\n",
    "\n",
    "def compile_tex_to_pdf(tex_filename):\n",
    "    \"\"\"Compile the .tex file to a PDF using pdflatex.\"\"\"\n",
    "    try:\n",
    "        # Use subprocess to call pdflatex and automatically respond to input requests\n",
    "        process = subprocess.Popen(\n",
    "            ['pdflatex', tex_filename],\n",
    "            stdin=subprocess.PIPE,   # Allow sending input to the process\n",
    "            stdout=subprocess.PIPE,   # Capture standard output\n",
    "            stderr=subprocess.PIPE,   # Capture standard error\n",
    "            text=True                 # Use text mode for input/output\n",
    "        )\n",
    "        \n",
    "        # Automatically send an Enter key press whenever it expects input\n",
    "        stdout, stderr = process.communicate(input='\\n')  # Sending newline (Enter)\n",
    "        \n",
    "        # Check the return code of the process\n",
    "        if process.returncode == 0:\n",
    "            print(f\"Successfully compiled {tex_filename} to PDF.\")\n",
    "        else:\n",
    "            print(f\"Error compiling {tex_filename} to PDF.\")\n",
    "            print(\"STDOUT:\", stdout)\n",
    "            print(\"STDERR:\", stderr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written LaTeX content to Workflow.tex\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the filename for the .tex file\n",
    "tex_filename = 'Workflow.tex'\n",
    "\n",
    "# Convert the string to a .tex file\n",
    "string_to_tex_file(response.text, tex_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully compiled Workflow.tex to PDF.\n"
     ]
    }
   ],
   "source": [
    "tex_filename = 'Workflow.tex'\n",
    "compile_tex_to_pdf(tex_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_form",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
