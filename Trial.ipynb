{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The President of the United States is **Joe Biden**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "os.environ[\"API_KEY\"] = 'AIzaSyC8hiLFFM8_tH7B05QHZMXmSpHWgM0HdFU'\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "\n",
    "response = model.generate_content(\"President of USA is\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Minkowski Metric in Special Relativity\n",
      "\n",
      "### Index\n",
      "\n",
      "1.  **The Minkowski Metric**\n",
      "    *   Definition and Significance\n",
      "    *   Matrix Representation\n",
      "2.  **Raising and Lowering Indices**\n",
      "    *   Contravariant and Covariant Components\n",
      "    *   Using the Minkowski Metric for Index Manipulation\n",
      "    *   Example: Raising and Lowering Indices\n",
      "3.  **Application in Lorentz Transformations and Commutators**\n",
      "    *   Lorentz Transformations and Index Manipulation\n",
      "    *   Commutators of Lorentz Generators\n",
      "4.  **Raising and Lowering in Tensor Equations**\n",
      "    *   Lorentz Generators and Commutators\n",
      "    *   Ensuring Symmetry in Spacetime\n",
      "5.  **Gamma Matrices and Index Raising/Lowering**\n",
      "    *   Dirac Adjoint Spinor\n",
      "    *   Lorentz Invariance of Spinor Bilinears\n",
      "    *   Raising and Lowering Indices of Gamma Matrices\n",
      "6.  **Summary**\n",
      "\n",
      "### 1. The Minkowski Metric\n",
      "\n",
      "#### Definition and Significance\n",
      "\n",
      "The Minkowski metric is a fundamental mathematical tool in special relativity used to describe the geometry of spacetime. It allows us to calculate distances (or intervals) in spacetime, a key concept in understanding the relationship between space and time in relativistic physics.\n",
      "\n",
      "#### Matrix Representation\n",
      "\n",
      "The Minkowski metric, denoted by \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " ‚Äã\n",
      " , is a 4x4 matrix that defines the geometry of spacetime in special relativity. For flat spacetime, it is typically represented as:\n",
      "\n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " ‚Äã\n",
      "  = diag(‚àí1,+1,+1,+1)\n",
      "\n",
      "This means:\n",
      "\n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " ‚Äã\n",
      "  = \n",
      "‚Äã\n",
      "  \n",
      " ‚àí1\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "‚Äã\n",
      "  \n",
      " 0\n",
      " +1\n",
      " 0\n",
      " 0\n",
      "‚Äã\n",
      "  \n",
      " 0\n",
      " 0\n",
      " +1\n",
      " 0\n",
      "‚Äã\n",
      "  \n",
      " 0\n",
      " 0\n",
      " 0\n",
      " +1\n",
      "‚Äã\n",
      "  \n",
      " \n",
      "The negative sign in the first entry corresponds to the time component, while the positive signs in the remaining diagonal entries represent the space components.\n",
      "\n",
      "### 2. Raising and Lowering Indices\n",
      "\n",
      "#### Contravariant and Covariant Components\n",
      "\n",
      "In special relativity and the Dirac equation, vectors and tensors have indices (superscripts or subscripts) that indicate their components. These indices correspond to the time and space components of the vector or tensor. \n",
      "\n",
      "*   **Upper indices (e.g.,  \n",
      " v\n",
      " Œº\n",
      "  )** represent contravariant components.\n",
      "*   **Lower indices (e.g.,  \n",
      " v\n",
      " Œº\n",
      "‚Äã\n",
      "  )** represent covariant components.\n",
      "\n",
      "#### Using the Minkowski Metric for Index Manipulation\n",
      "\n",
      "The Minkowski metric \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " ‚Äã\n",
      "  is used to convert between these two types of components, a process known as \"raising\" and \"lowering\" indices. This helps us switch between different forms of tensors (covariant and contravariant) which is crucial for performing calculations and expressing physical laws in a consistent manner.\n",
      "\n",
      "#### Example: Raising and Lowering Indices\n",
      "\n",
      "*   **Lowering an index:** To convert a contravariant vector ( \n",
      " v\n",
      " Œº\n",
      "  ) to a covariant vector ( \n",
      " v\n",
      " Œº\n",
      "‚Äã\n",
      "  ), we use the Minkowski metric:\n",
      "\n",
      " \n",
      " v\n",
      " Œº\n",
      "‚Äã\n",
      "  = Œ∑\n",
      " ŒºŒΩ\n",
      "‚Äã\n",
      "  \n",
      " v\n",
      " ŒΩ\n",
      " \n",
      "\n",
      "This means that each component of the lowered vector \n",
      " \n",
      " v\n",
      " Œº\n",
      "‚Äã\n",
      "  is a sum of products of the metric \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " ‚Äã\n",
      "  and the contravariant vector components \n",
      " \n",
      " v\n",
      " ŒΩ\n",
      " .\n",
      "\n",
      "*   **Raising an index:** To convert a covariant vector ( \n",
      " v\n",
      " Œº\n",
      "‚Äã\n",
      "  ) to a contravariant vector ( \n",
      " v\n",
      " Œº\n",
      "  ), we use the inverse of the Minkowski metric \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      "‚Äã\n",
      "  , denoted by \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " . In the case of the Minkowski metric, \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      "‚Äã\n",
      "  = \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " , so the same matrix is used for both raising and lowering indices:\n",
      "\n",
      " \n",
      " v\n",
      " Œº\n",
      "  = Œ∑\n",
      " ŒºŒΩ\n",
      "  \n",
      " v\n",
      " ŒΩ\n",
      "‚Äã\n",
      "  \n",
      "\n",
      "### 3. Application in Lorentz Transformations and Commutators\n",
      "\n",
      "#### Lorentz Transformations and Index Manipulation\n",
      "\n",
      "When working with Lorentz transformations, the Minkowski metric ensures that spacetime intervals and other physical quantities remain invariant. For example, under a Lorentz transformation, a contravariant vector \n",
      " \n",
      " v\n",
      " Œº\n",
      "  might transform as:\n",
      "\n",
      " \n",
      " v\n",
      " ‚Ä≤Œº\n",
      "  = Œõ\n",
      " ¬†ŒΩ\n",
      " Œº\n",
      "‚Äã\n",
      "  \n",
      " v\n",
      " ŒΩ\n",
      " \n",
      "\n",
      "where \n",
      " \n",
      " Œõ\n",
      " ¬†ŒΩ\n",
      " Œº\n",
      "‚Äã\n",
      "  is the Lorentz transformation matrix. To obtain the covariant version of the transformed vector, we lower the index using the Minkowski metric:\n",
      "\n",
      " \n",
      " v\n",
      " ‚Ä≤Œº\n",
      "‚Äã\n",
      "  = Œ∑\n",
      " ŒºŒΩ\n",
      "‚Äã\n",
      "  \n",
      " v\n",
      " ‚Ä≤ŒΩ\n",
      "  = Œ∑\n",
      " ŒºŒΩ\n",
      "‚Äã\n",
      "  Œõ\n",
      " ¬†œÉ\n",
      " ŒΩ\n",
      "‚Äã\n",
      "  \n",
      " v\n",
      " œÉ\n",
      " \n",
      "\n",
      "#### Commutators of Lorentz Generators\n",
      "\n",
      "In the context of Lorentz generators \n",
      " \n",
      " M\n",
      " ŒºŒΩ\n",
      " , the Minkowski metric appears in the commutator relations:\n",
      "\n",
      "[\n",
      " M\n",
      " ŒºŒΩ\n",
      " ,\n",
      " M\n",
      " œÅœÉ\n",
      " ] = i(Œ∑\n",
      " ŒΩœÅ\n",
      "  \n",
      " M\n",
      " ŒºœÉ\n",
      "  ‚àí Œ∑\n",
      " ŒºœÅ\n",
      "  \n",
      " M\n",
      " ŒΩœÉ\n",
      "  + Œ∑\n",
      " ŒºœÉ\n",
      "  \n",
      " M\n",
      " ŒΩœÅ\n",
      "  ‚àí Œ∑\n",
      " ŒΩœÉ\n",
      "  \n",
      " M\n",
      " ŒºœÅ\n",
      " )\n",
      "\n",
      "Here, the Minkowski metric helps to raise or lower indices, ensuring that the algebra correctly respects the symmetries of spacetime.\n",
      "\n",
      "### 4. Raising and Lowering in Tensor Equations\n",
      "\n",
      "When dealing with tensors, such as the Lorentz generators \n",
      " \n",
      " M\n",
      " ŒºŒΩ\n",
      " , we often need to raise or lower indices to contract tensors properly or apply Lorentz transformations. The Minkowski metric plays a crucial role in this process, ensuring that the mathematical manipulations preserve the fundamental symmetries of spacetime.\n",
      "\n",
      "### 5. Gamma Matrices and Index Raising/Lowering\n",
      "\n",
      "#### Dirac Adjoint Spinor\n",
      "\n",
      "In the context of the gamma matrices \n",
      " \n",
      " Œ≥\n",
      " Œº\n",
      "  used in the Dirac equation, the Minkowski metric also helps in raising and lowering indices. For example, the Dirac adjoint spinor \n",
      " \n",
      " œà\n",
      " Àâ\n",
      "‚Äã\n",
      "  is defined as:\n",
      "\n",
      " \n",
      " œà\n",
      " Àâ\n",
      "‚Äã\n",
      "  = œà\n",
      " ‚Ä†\n",
      "  Œ≥\n",
      " 0\n",
      " \n",
      "\n",
      "Here, \n",
      " \n",
      " Œ≥\n",
      " 0\n",
      "  is the time component of the gamma matrices, and it plays a key role in defining Lorentz invariance of the spinor bilinear \n",
      " \n",
      " œà\n",
      " Àâ\n",
      "‚Äã\n",
      "  œà.\n",
      "\n",
      "#### Lorentz Invariance of Spinor Bilinears\n",
      "\n",
      "To compute quantities like \n",
      " \n",
      " œà\n",
      " Àâ\n",
      "‚Äã\n",
      "  Œ≥\n",
      " Œº\n",
      "  œà, we might need to raise or lower the index on \n",
      " \n",
      " Œ≥\n",
      " Œº\n",
      "  using the Minkowski metric. This ensures that the resulting expressions transform correctly under Lorentz transformations, preserving the fundamental symmetries of spacetime.\n",
      "\n",
      "### 6. Summary\n",
      "\n",
      "The Minkowski metric \n",
      " \n",
      " Œ∑\n",
      " ŒºŒΩ\n",
      " ‚Äã\n",
      "  is a fundamental tool in special relativity that serves several crucial purposes:\n",
      "\n",
      "*   **Differentiating between time and space components in spacetime.**\n",
      "*   **Raising and lowering indices, switching between covariant and contravariant components of vectors and tensors.**\n",
      "*   **Ensuring that the symmetry properties of spacetime are respected in Lorentz transformations and commutators.**\n",
      "*   **Playing a vital role in the behavior of spinors and gamma matrices in the Dirac equation.**\n",
      "\n",
      "By using the Minkowski metric, we ensure that all physical laws remain Lorentz invariant, meaning they are the same in all inertial reference frames, a core principle of special relativity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Report on Command-Line Interface (CLI) Shell Implementation in C\n",
    "1. Introduction\n",
    "This C program implements a simple command-line interface (CLI) shell for Windows, similar to popular Unix/Linux shells such as Bash. The shell processes user input, manages command history, changes directories, lists files, and performs basic file system operations. The primary objective of this shell is to allow users to execute basic commands (cd, ls, pwd, echo, hist) and interact with the Windows operating system through a terminal interface.\n",
    "\n",
    "2. Program Structure and Components\n",
    "2.1. Libraries and Macros\n",
    "The program includes several standard libraries:\n",
    "\n",
    "<stdio.h>: Standard input/output functions.\n",
    "<stdlib.h>: Standard library functions like getenv() and strtok().\n",
    "<string.h>: Functions for string manipulation (strncpy(), strcmp()).\n",
    "<windows.h>: Windows-specific system calls (GetComputerName(), FindFirstFile()).\n",
    "<direct.h>: For directory-related functions like _getcwd() and _chdir().\n",
    "<limits.h>: For PATH_MAX (maximum allowable path length).\n",
    "Macros:\n",
    "\n",
    "MAX_INPUT: Defines the maximum size for input (1024 characters).\n",
    "MAX_ARGS: Sets the maximum number of command arguments (64).\n",
    "MAX_HISTORY: Sets the size of the command history buffer (10).\n",
    "2.2. Command History Management\n",
    "The program maintains a history of the last 10 commands executed by the user.\n",
    "\n",
    "Global Variables:\n",
    "history[MAX_HISTORY][MAX_INPUT]: Stores the history of the last 10 commands.\n",
    "history_count: Tracks the current number of commands in the history.\n",
    "Functions:\n",
    "\n",
    "add_to_history(const char *command): Adds the latest command to the history. If the history buffer is full, the oldest command is removed, and the new command is added.\n",
    "show_history(): Displays the command history to the user.\n",
    "2.3. Directory Navigation\n",
    "The program provides functionality to change and print the current directory.\n",
    "\n",
    "Functions:\n",
    "\n",
    "change_directory(char *path, char *prev_dir): Changes the current working directory. If no path is provided or the user specifies \"~\", it defaults to the user's home directory. If the user enters \"-\", the program switches to the previous directory.\n",
    "print_working_directory(): Prints the current working directory using _getcwd().\n",
    "2.4. File Listing\n",
    "list_files(): Lists the files in the current directory using Windows API functions. It leverages FindFirstFile() and FindNextFile() to traverse files and directories in the current location.\n",
    "2.5. Command Parsing and Execution\n",
    "The shell reads user input, tokenizes the command, and executes appropriate actions.\n",
    "\n",
    "Input Parsing:\n",
    "\n",
    "The shell continuously waits for user input via fgets().\n",
    "The input is split into tokens using strtok(), and each token is stored in the args array.\n",
    "Commands are matched using strcmp() and executed based on a set of predefined commands.\n",
    "Supported Commands:\n",
    "\n",
    "cd: Changes the current directory. Handles special cases like cd - (switch to the previous directory) and cd ~ (switch to the home directory).\n",
    "ls: Lists files in the current directory.\n",
    "pwd: Prints the current working directory.\n",
    "echo: Prints the provided arguments to the terminal.\n",
    "hist: Displays the command history.\n",
    "exit: Exits the shell program.\n",
    "2.6. Command Execution Workflow\n",
    "Prompt Display: The shell prints a prompt in the format [username@hostname cwd]$, where:\n",
    "\n",
    "username: Retrieved from the environment variable USERNAME.\n",
    "hostname: Obtained using the Windows function GetComputerName().\n",
    "cwd: The current working directory, fetched using _getcwd().\n",
    "Input Handling:\n",
    "\n",
    "The shell reads user input and processes it with fgets().\n",
    "If the command is not empty, it is added to the history using add_to_history().\n",
    "Command Execution:\n",
    "\n",
    "The shell tokenizes the input into commands and arguments.\n",
    "It checks for each supported command (cd, ls, pwd, echo, etc.) and invokes the corresponding function.\n",
    "If an unsupported command is entered, the shell prints an error message.\n",
    "Looping: The program continuously loops until the user types exit.\n",
    "\n",
    "3. Error Handling\n",
    "The shell performs basic error handling:\n",
    "\n",
    "If getcwd() or chdir() fails, it prints an appropriate error message.\n",
    "If too many arguments are passed to the cd command, the program prints an error and ignores the command.\n",
    "The FindFirstFile() function handles cases where the directory cannot be read.\n",
    "4. Key Functions\n",
    "change_directory(char *path, char *prev_dir): Handles changing directories and supports navigating to the home directory (~) and switching to the previous directory (-).\n",
    "\n",
    "list_files(): Uses Windows API functions to list files and directories in the current location.\n",
    "\n",
    "echo_command(char *input): This function prints the user‚Äôs input to the terminal, implementing an echo command.\n",
    "\n",
    "5. Conclusion\n",
    "This program demonstrates how a simple command-line shell can be implemented in C on a Windows system. It includes key features like command history, directory navigation, file listing, and the ability to execute basic commands. The use of Windows API functions enables interaction with the file system, while standard C functions handle input/output and string manipulation.\n",
    "\n",
    "The shell could be further extended by adding support for more commands and features like piping, file redirection, or job control to make it more powerful.'''\n",
    "\n",
    "query = '''Your task is to covert the following text into \n",
    "markdown format with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "\n",
    "Also before this first write index in markdown as well with appropriate headings and subheadings.\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "\n",
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_string_to_markdown(input_string, file_name):\n",
    "    # Ensure the file name has a .md extension\n",
    "    if not file_name.endswith('.md'):\n",
    "        file_name += '.md'\n",
    "    \n",
    "    # Open the file in write mode and write the string to it\n",
    "    with open(file_name, 'w', encoding='utf-8') as md_file:\n",
    "        md_file.write(input_string)\n",
    "    \n",
    "    print(f\"String successfully saved to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Report_osa.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_string_to_markdown(response.text, 'Report_osa.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesRetrieval Augmented \n",
      "Generation (RAG)\n",
      "Umar Jamil\n",
      "Downloaded from: https://github.com/hkproj/retrieval -augmented -generation -notes\n",
      "License : Creative Commons Attribution -NonCommercial 4.0 International (CC BY -NC 4.0): \n",
      "https://creativecommons.org/licenses/by -nc/4.0/legalcode\n",
      "Not for commercial use\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Prerequisites\n",
      "‚Ä¢Structure of the Transformer model and how the attention mechanism works.\n",
      "‚Ä¢BERT  (MLM task, [ cls] token)\n",
      "Extra\n",
      "‚Ä¢As usual, my cat Â••Âà©Â•• will also be part of this video.\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWhat is a language model?\n",
      "A language model is a probabilistic model that assign probabilities to sequence of words.\n",
      "In practice, a language model allows us to compute the following:\n",
      "We usually train a neural network to predict these probabilities. A neural network trained on a \n",
      "large corpora of text is known as a Large Language Model (LLM).P [ ‚ÄúChina‚Äù | ‚ÄúShanghai is a city in‚Äù ]\n",
      "Prompt Next TokenUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesHow do we train and inference a Language Model?\n",
      "Training\n",
      "A language model is trained on a corpora of text, that is, a large collection of documents. Often, Language Models are \n",
      "trained on the entire Wikipedia and millions of web pages. This allows the Language Model to acquire as much knowledge \n",
      "as possible.\n",
      "We usually train a Transformer -based neural network as Language Model.\n",
      "Inference\n",
      "To inference a Language Model, we build a prompt and let the Language Model generate the rest by iteratively adding tokens.\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesYou are what you eat\n",
      "A language model can only output text and information that it was trained upon. This means, \n",
      "that if we train a language model only on English content, very probably it won‚Äôt be able to \n",
      "output Japanese or French. To teach new concepts, we need to fine -tune the model. Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesThe cons of fine -tuning\n",
      "‚Ä¢It can be expensive.\n",
      "‚Ä¢The number of parameters of the model may not be sufficient to capture all the knowledge \n",
      "we want to teach to it. That‚Äôs why LLaMA  was introduced with 7B, 13B and 70B parameters. \n",
      "‚Ä¢Fine -Tuning is not additive. It may replace existing knowledge of the model with new \n",
      "knowledge. For example, a language model trained on English that is (heavily) fine -tuned on \n",
      "Japanese may ‚Äúforget‚Äù English.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesPrompt Engineering to the rescue!\n",
      "It is possible to ‚Äúteach ‚Äù a language model how to perform a new task by playing with the \n",
      "prompt. For example, by using ‚Äúfew-shot ‚Äù prompting. The following is an example:\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesQA with Prompt Engineering\n",
      "Context\n",
      "AnswerQuestionPromptInstructionsUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesThe pros of fine -tuning\n",
      "‚Ä¢Higher quality results compared to prompt engineering.\n",
      "‚Ä¢Smaller context size (input size) during inference since we don ‚Äôt need to include the context \n",
      "and instructions.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesQA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWhy do we use vectors to represent words?\n",
      "Given the words ‚Äú cherry ‚Äù, ‚Äúdigital ‚Äù and ‚Äú information ‚Äù, if we represent the embedding vectors \n",
      "using only 2 dimensions (X, Y) and we plot them, we hope to see something like this: the angle \n",
      "between words with similar meaning is small, while the angle between words with different \n",
      "meaning is big. So, the embeddings ‚Äúcapture‚Äù the meaning of the words they represent by \n",
      "projecting them into a high -dimensional space.\n",
      "Source: Speech and Language Processing 3rd Edition Draft, Dan Jurafsky  and James H. Martin\n",
      "We commonly use the cosine similarity , which is based on the dot product between the two \n",
      "vectors.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWord embeddings: the ideas\n",
      "‚Ä¢Words that are synonyms tend to occur in the same context (surrounded by the same \n",
      "words). \n",
      "‚Ä¢For example, the word ‚Äúteacher ‚Äù and ‚Äúprofessor ‚Äù usually occur surrounded by the \n",
      "words ‚Äúschool ‚Äù,  ‚Äúuniversity ‚Äù, ‚Äúexam ‚Äù, ‚Äúlecture ‚Äù, ‚Äúcourse ‚Äù, etc..\n",
      "‚Ä¢The inverse can also be true: words that occur in the same context tend to have similar \n",
      "meanings. This is known as the distributional hypothesis .\n",
      "‚Ä¢This means that to capture the meaning of the word, we also need to have access to its \n",
      "context (the words surrounding it).\n",
      "‚Ä¢This is why we employ the Self -Attention mechanism in the Transformer model to capture \n",
      "contextual information for every token. The Self -Attention mechanism relates every token to \n",
      "all the other tokens in the sentence.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesWord embeddings: the Cloze task\n",
      "‚Ä¢Imagine I give you the following sentence: \n",
      "Rome is the _______  of Italy, which is why it hosts many government buildings. \n",
      "Can you tell me what is the missing word?\n",
      "‚Ä¢Of course! The missing word is ‚Äúcapital ‚Äù, because by looking at the rest of the sentence, it is \n",
      "the one that makes the most sense.\n",
      "‚Ä¢This is how we train BERT: we want the Self -Attention mechanism to relate all the input \n",
      "tokens with each other, so that BERT has enough information about the ‚Äúcontext ‚Äù of the \n",
      "missing word to predict it.Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesHow do we train embedding vectors in BERT?\n",
      "Input  (14 tokens):Output  (14 tokens):Target  (1 token): capital\n",
      "Run backpropagation  to update the weights Loss\n",
      "Rome is the [mask]  of Italy, which is why it hosts many government buildings.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13 TK14Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings\n",
      "‚Ä¢We can use the Self -Attention mechanism also to capture the ‚Äúmeaning‚Äù of an entire \n",
      "sentence.\n",
      "‚Ä¢We can use a pre -trained BERT model to produce embeddings of entire sentences. Let‚Äôs \n",
      "see howUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings with BERT\n",
      "Input  (13 tokens):Output  (13 tokens):\n",
      "Our professor always gives us lots of assignments to do in the weekend.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13\n",
      "Output of the Self -Attention : matrix of shape (13, 768)\n",
      "Input of the Self -Attention : matrix of shape (13, 768)Mean -Pooling: take the average of all the vectorsSentence EmbeddingSingle vector with 768 dimensionsUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence Embeddings: comparison\n",
      "‚Ä¢How can we compare Sentence Embeddings to see if two sentences have similar \n",
      "‚Äúmeaning ‚Äù? We could use the cosine similarity, which measures the cosine of the angle \n",
      "between the two vectors. A small angle results in a high cosine similarity score.\n",
      "‚Ä¢But there‚Äôs a problem : nobody told BERT that the embeddings it produces should be \n",
      "comparable with the cosine similarity, that is, two similar sentences should be represented \n",
      "by vectors pointing to the same direction in space. How can we teach BERT to produce \n",
      "embeddings that can be compared with a similarity function of our choice?Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesIntroducing Sentence BERT\n",
      "Umar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesSentence BERT: architecture\n",
      "Sentence A Sentence BBERTPoolingSentence Embedding A\n",
      "BERTPoolingSentence Embedding B\n",
      "Same architecture\n",
      "Same parameters\n",
      "Same weightsCosine SimilarityTarget  (Cosine Similarity)\n",
      "Loss (MSE)\n",
      "We can apply a linear layer to \n",
      "reduce the size of the \n",
      "embedding vector, for \n",
      "example from 768 to 512We can use mean -pooling, \n",
      "max -pooling or just use the [ cls] \n",
      "token as sentence embedding.Siamese Network\n",
      "‚ÄúMy father plays with my me at the park‚Äù ‚ÄúI play with my dad at the park ‚ÄùUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesQA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesStrategies to teach new concepts to LLM\n",
      "Base LLM\n",
      "Fine -tune on custom data\n",
      "Fine -tuned LLM+\n",
      "=Base LLM\n",
      "Vector DB + Embeddings\n",
      "RAG+\n",
      "=\n",
      "Fine -tuned LLM + RAGOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Vector DB: introduction\n",
      "A vector database stores vectors of fixed dimensions (called embeddings) such that we can then query the \n",
      "database to find all the embeddings that are closest (most similar) to a given query vector using a distance \n",
      "metric, which is usually the cosine similarity, but we can also use the Euclidean distance. The database uses \n",
      "a variant of the KNN (K Nearest Neighbor) algorithm or another similarity search algorithm. Vector DBs are \n",
      "also used for finding similar songs (e.g. Spotify), images (e.g. Google Images) or products (e.g. Amazon).\n",
      "Query\n",
      "Vector DBWeb Pages DocumentsHow many parameters are there in Grok -0?\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with 33 billion parameters. This early model approaches \n",
      "LLaMA  2 (70B) capabilities on standard LM benchmarks but uses only half of its training resources. \n",
      "[...]Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KK-NN: a na√Øve approach\n",
      "Imagine we want to search for the query in our database: a simple way would be comparing \n",
      "the query with all the vectors, sorting them by distance, and keeping the top K.\n",
      "QueryHow many parameters are there in Grok -0?Emb . 1\n",
      "Emb . 2\n",
      "Emb . 3\n",
      "‚Ä¶\n",
      "Emb . 1,000,000‚Ä¶\n",
      "‚Ä¶\n",
      "‚Ä¶\n",
      "‚Ä¶\n",
      "Emb . 999,998\n",
      "Emb . 999,999Distance Vector\n",
      "0.3\n",
      "0.1\n",
      "0.2\n",
      "0.9\n",
      "0.2\n",
      "0.70.4\n",
      "0.0\n",
      "0.1\n",
      "0.2\n",
      "0.6Keep top KIf there are N embedding vectors and each \n",
      "has D dimensions, the computational \n",
      "complexity is in the order of O(N*D), too \n",
      "slow!üêåOutline\n",
      "‚Ä¢Intro to Large Language Models\n",
      "‚Ä¢RAG pipeline\n",
      "‚Ä¢Embedding vectors\n",
      "‚Ä¢Sentence BERT\n",
      "‚Ä¢Vector DB\n",
      "‚Ä¢Algorithms (HNSW)Similarity Search: let‚Äôs trade precision for speed\n",
      "The na√Øve approach we used before, always produces accurate \n",
      "results, since it compares the query with all the stored vectors, but \n",
      "what if we reduced the number of comparison, but still obtain \n",
      "accurate results with high probability?\n",
      "The metric we usually care about in Similarity Search is recall.\n",
      "In this video we will explore an algorithm for Approximate Nearest \n",
      "Neighbors, called Hierarchical Navigable Small Worlds (HNSW) . \n",
      "Source: WikipediaHNSW in the real world\n",
      "It is the same algorithm that powers Qdrant , the open source Vector DB used by Twitter‚Äôs  (X) Grok  LLM, \n",
      "which can access tweets in real time.\n",
      "HNSW: idea # 1\n",
      "HNSW is an evolution of the Navigable Small Worlds  algorithm for Approximate Nearest Neighbors, which \n",
      "is based on the concept of Six Degrees of Separation .\n",
      "Milgram's experiment aimed to test the social connections among people in the United States. The \n",
      "participants, who were initially located in Nebraska and Kansas, were given a letter to be delivered to a \n",
      "specific person in Boston. However, they were not allowed to send the letter directly to the recipient. \n",
      "Instead, they were instructed to send it to someone they knew on a first -name basis, who they believed \n",
      "might have a better chance of knowing the target person.\n",
      "At the end of Milgram ‚Äôs small -world experiment, Milgram found that most of the letters reached the final \n",
      "recipient in five or six steps, creating the concept that people all over the world are all connected by six \n",
      "degrees of separation.\n",
      "Facebook found in 2016 that its 1.59 billion active users were connected on average by 3.5 degrees of \n",
      "separation: https://research.facebook.com/blog/ 2016 /02/three -and-a-half-degrees -of-separation/\n",
      "This means that you and Mark Zuckerberg are only 3.5 connections apart!Navigable Small Worlds\n",
      "The NSW algorithm builds a graph that ‚Äì just like Facebook friends ‚Äì connects close vectors with each other \n",
      "but keeping the total number of connections small. For example, every vector may be connected to up to 6 \n",
      "other vectors (to mimic the Six Degrees of Separation).\n",
      "0102\n",
      "0305\n",
      "04\n",
      "0609\n",
      "0813\n",
      "11\n",
      "121510 07\n",
      "14Node Text\n",
      "01 [‚Ä¶] The Transformer is a model [‚Ä¶]\n",
      "02 [‚Ä¶] Diagnose cancer with AI [‚Ä¶]\n",
      "03 [‚Ä¶] A transformer -based model [‚Ä¶]\n",
      "04 [‚Ä¶] The Transformer has 6 layers [‚Ä¶]\n",
      "05 [‚Ä¶] An MRI machine that costs 1$ [‚Ä¶]\n",
      "06 [‚Ä¶] The dot -product is a [‚Ä¶]\n",
      "07 [‚Ä¶] Big -Pharma is not so big [‚Ä¶]\n",
      "08 [‚Ä¶] Cross -Attention is a great [‚Ä¶]\n",
      "09 [‚Ä¶] To solve an ODE [‚Ä¶]\n",
      "10 [‚Ä¶] We are aging too fast [‚Ä¶]\n",
      "11 [‚Ä¶] Open -source models like [‚Ä¶]\n",
      "12 [‚Ä¶] MathBERT : a new model [‚Ä¶]\n",
      "13 [‚Ä¶] AI to control aging [‚Ä¶]\n",
      "14 [‚Ä¶] Attention is all you need [‚Ä¶]\n",
      "15 [‚Ä¶] LLaMA  2 has 7B params [‚Ä¶]Navigable Small Worlds: searching for K -NN\n",
      "Given the following query: ‚Äú How many Encoder layers are there in the Transformer model? ‚Äù\n",
      "How does the algorithm find the K Nearest Neighbors?\n",
      "0102\n",
      "0305\n",
      "04\n",
      "0609\n",
      "0813\n",
      "11\n",
      "121510 07\n",
      "14Node Text\n",
      "01 [‚Ä¶] The Transformer is a model [‚Ä¶]\n",
      "02 [‚Ä¶] Diagnose cancer with AI [‚Ä¶]\n",
      "03 [‚Ä¶] A transformer -based model [‚Ä¶]\n",
      "04 [‚Ä¶] The Transformer has 6 layers [‚Ä¶]\n",
      "05 [‚Ä¶] An MRI machine that costs 1$ [‚Ä¶]\n",
      "06 [‚Ä¶] The dot -product is a [‚Ä¶]\n",
      "07 [‚Ä¶] Big -Pharma is not so big [‚Ä¶]\n",
      "08 [‚Ä¶] Cross -Attention is a great [‚Ä¶]\n",
      "09 [‚Ä¶] To solve an ODE [‚Ä¶]\n",
      "10 [‚Ä¶] We are aging too fast [‚Ä¶]\n",
      "11 [‚Ä¶] Open -source models like [‚Ä¶]\n",
      "12 [‚Ä¶] MathBERT : a new model [‚Ä¶]\n",
      "13 [‚Ä¶] AI to control aging [‚Ä¶]\n",
      "14 [‚Ä¶] Attention is all you need [‚Ä¶]\n",
      "15 [‚Ä¶] LLaMA  2 has 7B params [‚Ä¶]Q\n",
      "Local best: stop search\n",
      "We repeat the search with randomly chosen starting \n",
      "points and then keep the top K among all the visited \n",
      "nodes.Navigable Small Worlds: inserting a new vector\n",
      "We can insert a new vector by searching the top KNN with the searching algorithm described before and \n",
      "making an edge between the vector and the top K results.HNSW: idea #2\n",
      "To go from NSW (Navigable Small Worlds) to HNSW (Hierarchical Navigable Small Worlds), we need to \n",
      "introduce the algorithm behind the data structure known as Skip -List.\n",
      "The skip list is a data structure that maintains a sorted list and allows search and insertion with an average of \n",
      "ùëÇ(logùëÅ) time complexity.\n",
      "H3\n",
      "H2\n",
      "H1\n",
      "H0 EEEE\n",
      "1 5 3 9 12 17 23Let‚Äôs search the number 9HNSW: Hierarchical Navigable Small Worlds\n",
      "HNSW: Hierarchical Navigable Small Worlds\n",
      "Q\n",
      "Layer 0 (dense)Layer 3 (sparse)\n",
      "Layer 1Layer 2Let‚Äôs search!\n",
      "We repeat the search with \n",
      "randomly chosen starting \n",
      "points (on the top layer) and \n",
      "then keep the top K among \n",
      "all the visited nodes.QA with Retrieval Augmented Generation\n",
      "Query\n",
      "Vector DBWeb Pages Documents\n",
      "LLM\n",
      "How many parameters are there in Grok -0?\n",
      "Prompt\n",
      "Template\n",
      "Context\n",
      "[...]\n",
      "After announcing xAI, we trained a prototype LLM (Grok -0) with \n",
      "33 billion parameters. This early model approaches LLaMA  2 \n",
      "(70B) capabilities on standard LM benchmarks but uses only \n",
      "half of its training resources. \n",
      "[...]Answer\n",
      "Grok -0, the prototype LLM \n",
      "mentioned in the provided \n",
      "context, is stated to have been \n",
      "trained with 33 billion parameters.Split into chunks\n",
      "EmbeddingsEmbeddings\n",
      "StoreSearch\n",
      "Top-KUmar Jamil ‚Äìhttps://github.com/hkproj/retrieval -augmented -generation -notesThanks for watching!\n",
      "Don‚Äôt forget to subscribe for \n",
      "more amazing content on AI \n",
      "and Machine Learning!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file):\n",
    "    # Open the PDF file\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        # Initialize a variable to store the extracted text\n",
    "        extracted_text = \"\"\n",
    "        \n",
    "        # Iterate through all the pages and extract text\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            extracted_text += page.extract_text()\n",
    "        \n",
    "        return extracted_text\n",
    "\n",
    "# Example usage\n",
    "pdf_file = 'RAG.pdf'  # Specify your PDF file path here\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Retrieval Augmented Generation (RAG)\n",
      "\n",
      "## Index\n",
      "\n",
      "### 1. Introduction\n",
      "   - What is a Language Model?\n",
      "   - How do we train and inference a Language Model?\n",
      "   - The cons of fine-tuning\n",
      "   - Prompt Engineering\n",
      "   - QA with Prompt Engineering\n",
      "   - The pros of fine-tuning\n",
      "   - QA with Retrieval Augmented Generation\n",
      "\n",
      "### 2. Embedding Vectors\n",
      "   - Why do we use vectors to represent words?\n",
      "   - Word embeddings: the ideas\n",
      "   - Word embeddings: the Cloze task\n",
      "   - How do we train embedding vectors in BERT?\n",
      "   - Sentence Embeddings\n",
      "   - Sentence Embeddings with BERT\n",
      "   - Sentence Embeddings: comparison\n",
      "   - Introducing Sentence BERT\n",
      "   - Sentence BERT: architecture\n",
      "   - Strategies to teach new concepts to LLM\n",
      "\n",
      "### 3. Vector DB\n",
      "   - Vector DB: introduction\n",
      "   - K-NN: a na√Øve approach\n",
      "   - Similarity Search: let's trade precision for speed\n",
      "   - HNSW in the real world\n",
      "   - HNSW: idea #1\n",
      "   - Navigable Small Worlds\n",
      "   - Navigable Small Worlds: searching for K-NN\n",
      "   - Navigable Small Worlds: inserting a new vector\n",
      "   - HNSW: idea #2\n",
      "   - HNSW: Hierarchical Navigable Small Worlds\n",
      "   - HNSW: Hierarchical Navigable Small Worlds\n",
      "\n",
      "## Retrieval Augmented Generation (RAG)\n",
      "\n",
      "### Intro to Large Language Models\n",
      "\n",
      "A language model is a probabilistic model that assigns probabilities to sequences of words. \n",
      "\n",
      "In practice, a language model allows us to compute the following: \n",
      "\n",
      "```\n",
      "P [ \"China\" | \"Shanghai is a city in\" ]\n",
      "```\n",
      "\n",
      "We usually train a neural network to predict these probabilities. A neural network trained on a large corpus of text is known as a Large Language Model (LLM). \n",
      "\n",
      "### RAG Pipeline\n",
      "\n",
      "The RAG pipeline consists of the following steps:\n",
      "\n",
      "1. **Split into chunks:** The input document is split into smaller chunks.\n",
      "2. **Embeddings:** Each chunk is converted into an embedding vector using Sentence BERT.\n",
      "3. **Store:** The embedding vectors are stored in a vector database.\n",
      "4. **Search:** When a query is given, its embedding vector is calculated and used to search the vector database for the most similar chunks.\n",
      "5. **Top-K:** The top-K most similar chunks are retrieved from the vector database.\n",
      "6. **LLM:** The retrieved chunks are fed into a large language model (LLM) along with the query.\n",
      "7. **Prompt:** The LLM generates a response based on the query and the retrieved context.\n",
      "\n",
      "### Embedding Vectors\n",
      "\n",
      "#### Why do we use vectors to represent words?\n",
      "\n",
      "Given the words \"cherry\", \"digital\" and \"information\", if we represent the embedding vectors using only 2 dimensions (X, Y) and we plot them, we hope to see something like this: the angle between words with similar meaning is small, while the angle between words with different meaning is big. So, the embeddings \"capture\" the meaning of the words they represent by projecting them into a high-dimensional space. \n",
      "\n",
      "We commonly use the cosine similarity, which is based on the dot product between the two vectors.\n",
      "\n",
      "#### Word embeddings: the ideas\n",
      "\n",
      "- Words that are synonyms tend to occur in the same context (surrounded by the same words). \n",
      "- For example, the word \"teacher\" and \"professor\" usually occur surrounded by the words \"school\", \"university\", \"exam\", \"lecture\", \"course\", etc..\n",
      "- The inverse can also be true: words that occur in the same context tend to have similar meanings. This is known as the **distributional hypothesis**.\n",
      "- This means that to capture the meaning of the word, we also need to have access to its context (the words surrounding it).\n",
      "- This is why we employ the Self-Attention mechanism in the Transformer model to capture contextual information for every token. The Self-Attention mechanism relates every token to all the other tokens in the sentence.\n",
      "\n",
      "#### Word embeddings: the Cloze task\n",
      "\n",
      "- Imagine I give you the following sentence: \n",
      "  Rome is the _______  of Italy, which is why it hosts many government buildings. \n",
      "  Can you tell me what is the missing word?\n",
      "- Of course! The missing word is \"capital\", because by looking at the rest of the sentence, it is the one that makes the most sense.\n",
      "- This is how we train BERT: we want the Self-Attention mechanism to relate all the input tokens with each other, so that BERT has enough information about the \"context\" of the missing word to predict it.\n",
      "\n",
      "#### How do we train embedding vectors in BERT?\n",
      "\n",
      "```\n",
      "Input  (14 tokens):  Output  (14 tokens):  Target  (1 token): capital\n",
      "Rome is the [mask]  of Italy, which is why it hosts many government buildings.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13 TK14\n",
      "```\n",
      "\n",
      "We run backpropagation to update the weights and minimize the loss.\n",
      "\n",
      "#### Sentence Embeddings\n",
      "\n",
      "We can use the Self-Attention mechanism also to capture the \"meaning\" of an entire sentence. We can use a pre-trained BERT model to produce embeddings of entire sentences. \n",
      "\n",
      "#### Sentence Embeddings with BERT\n",
      "\n",
      "```\n",
      "Input  (13 tokens):  Output  (13 tokens):\n",
      "Our professor always gives us lots of assignments to do in the weekend.\n",
      "TK1 TK2 TK3 TK4 TK5 TK6 TK7 TK8 TK9 TK10 TK11 TK12 TK13\n",
      "```\n",
      "\n",
      "The output of the Self-Attention is a matrix of shape (13, 768). We can then apply mean-pooling to take the average of all the vectors, resulting in a single vector with 768 dimensions - our sentence embedding.\n",
      "\n",
      "#### Sentence Embeddings: comparison\n",
      "\n",
      "How can we compare Sentence Embeddings to see if two sentences have similar \"meaning\"? We could use the cosine similarity, which measures the cosine of the angle between the two vectors. A small angle results in a high cosine similarity score. \n",
      "\n",
      "But there's a problem: nobody told BERT that the embeddings it produces should be comparable with the cosine similarity, that is, two similar sentences should be represented by vectors pointing to the same direction in space. How can we teach BERT to produce embeddings that can be compared with a similarity function of our choice? \n",
      "\n",
      "### Introducing Sentence BERT\n",
      "\n",
      "Sentence BERT is a variation of BERT that is specifically trained to produce sentence embeddings that are comparable using cosine similarity. It does this by using a Siamese network.\n",
      "\n",
      "#### Sentence BERT: architecture\n",
      "\n",
      "Sentence BERT uses two identical BERT models, one for each input sentence. The outputs of the BERT models are then fed into a pooling layer to produce sentence embeddings. The sentence embeddings are then compared using cosine similarity, and the loss is calculated based on the difference between the cosine similarity and the target similarity score.\n",
      "\n",
      "```\n",
      "Sentence A\n",
      "BERT\n",
      "Pooling\n",
      "Sentence Embedding A\n",
      "Sentence B\n",
      "BERT\n",
      "Pooling\n",
      "Sentence Embedding B\n",
      "Cosine Similarity\n",
      "Target  (Cosine Similarity)\n",
      "Loss (MSE)\n",
      "```\n",
      "\n",
      "We can apply a linear layer to reduce the size of the embedding vector, for example from 768 to 512. We can use mean-pooling, max-pooling or just use the [cls] token as sentence embedding.\n",
      "\n",
      "#### Strategies to teach new concepts to LLM\n",
      "\n",
      "There are two main strategies for teaching new concepts to an LLM:\n",
      "\n",
      "1. **Fine-tuning:** The LLM is fine-tuned on a custom dataset that contains the desired knowledge. This can result in higher quality results compared to prompt engineering, but it can be expensive and may not be additive.\n",
      "2. **RAG:** The LLM is combined with a vector database that contains relevant context. This allows the LLM to access information from a wider range of sources, but it may not be as effective as fine-tuning for specific tasks.\n",
      "\n",
      "### Vector DB\n",
      "\n",
      "#### Vector DB: introduction\n",
      "\n",
      "A vector database stores vectors of fixed dimensions (called embeddings) such that we can then query the database to find all the embeddings that are closest (most similar) to a given query vector using a distance metric, which is usually the cosine similarity, but we can also use the Euclidean distance. The database uses a variant of the KNN (K Nearest Neighbor) algorithm or another similarity search algorithm. Vector DBs are also used for finding similar songs (e.g. Spotify), images (e.g. Google Images) or products (e.g. Amazon).\n",
      "\n",
      "#### K-NN: a na√Øve approach\n",
      "\n",
      "Imagine we want to search for the query in our database: a simple way would be comparing the query with all the vectors, sorting them by distance, and keeping the top K.\n",
      "\n",
      "If there are N embedding vectors and each has D dimensions, the computational complexity is in the order of O(N*D), too slow!üêå\n",
      "\n",
      "#### Similarity Search: let's trade precision for speed\n",
      "\n",
      "The na√Øve approach we used before, always produces accurate results, since it compares the query with all the stored vectors, but what if we reduced the number of comparison, but still obtain accurate results with high probability?\n",
      "\n",
      "The metric we usually care about in Similarity Search is recall.\n",
      "\n",
      "In this video we will explore an algorithm for Approximate Nearest Neighbors, called Hierarchical Navigable Small Worlds (HNSW). \n",
      "\n",
      "#### HNSW in the real world\n",
      "\n",
      "It is the same algorithm that powers Qdrant, the open source Vector DB used by Twitter‚Äôs (X) Grok LLM, which can access tweets in real time.\n",
      "\n",
      "#### HNSW: idea #1\n",
      "\n",
      "HNSW is an evolution of the Navigable Small Worlds algorithm for Approximate Nearest Neighbors, which is based on the concept of **Six Degrees of Separation**.\n",
      "\n",
      "Milgram's experiment aimed to test the social connections among people in the United States. The participants, who were initially located in Nebraska and Kansas, were given a letter to be delivered to a specific person in Boston. However, they were not allowed to send the letter directly to the recipient. Instead, they were instructed to send it to someone they knew on a first-name basis, who they believed might have a better chance of knowing the target person.\n",
      "\n",
      "At the end of Milgram‚Äôs small-world experiment, Milgram found that most of the letters reached the final recipient in five or six steps, creating the concept that people all over the world are all connected by six degrees of separation.\n",
      "\n",
      "Facebook found in 2016 that its 1.59 billion active users were connected on average by 3.5 degrees of separation: https://research.facebook.com/blog/2016/02/three-and-a-half-degrees-of-separation/\n",
      "\n",
      "This means that you and Mark Zuckerberg are only 3.5 connections apart!\n",
      "\n",
      "#### Navigable Small Worlds\n",
      "\n",
      "The NSW algorithm builds a graph that ‚Äì just like Facebook friends ‚Äì connects close vectors with each other but keeping the total number of connections small. For example, every vector may be connected to up to 6 other vectors (to mimic the Six Degrees of Separation).\n",
      "\n",
      "```\n",
      "01  02\n",
      "     \\\n",
      "      03\n",
      "     / \\\n",
      "    05  04\n",
      "   /  / \\\n",
      "  06  09  08\n",
      "       \\ /\n",
      "        13\n",
      "       / \\\n",
      "      11  12\n",
      "     /  \\  /\n",
      "    15  10 07\n",
      "        \\ /\n",
      "         14\n",
      "```\n",
      "\n",
      "#### Navigable Small Worlds: searching for K-NN\n",
      "\n",
      "Given the following query: ‚ÄúHow many Encoder layers are there in the Transformer model?‚Äù\n",
      "\n",
      "How does the algorithm find the K Nearest Neighbors?\n",
      "\n",
      "```\n",
      "01  02\n",
      "     \\\n",
      "      03\n",
      "     / \\\n",
      "    05  04\n",
      "   /  / \\\n",
      "  06  09  08\n",
      "       \\ /\n",
      "        13\n",
      "       / \\\n",
      "      11  12\n",
      "     /  \\  /\n",
      "    15  10 07\n",
      "        \\ /\n",
      "         14\n",
      "\n",
      "Node Text\n",
      "01 [‚Ä¶] The Transformer is a model [‚Ä¶]\n",
      "02 [‚Ä¶] Diagnose cancer with AI [‚Ä¶]\n",
      "03 [‚Ä¶] A transformer-based model [‚Ä¶]\n",
      "04 [‚Ä¶] The Transformer has 6 layers [‚Ä¶]\n",
      "05 [‚Ä¶] An MRI machine that costs 1$ [‚Ä¶]\n",
      "06 [‚Ä¶] The dot-product is a [‚Ä¶]\n",
      "07 [‚Ä¶] Big-Pharma is not so big [‚Ä¶]\n",
      "08 [‚Ä¶] Cross-Attention is a great [‚Ä¶]\n",
      "09 [‚Ä¶] To solve an ODE [‚Ä¶]\n",
      "10 [‚Ä¶] We are aging too fast [‚Ä¶]\n",
      "11 [‚Ä¶] Open-source models like [‚Ä¶]\n",
      "12 [‚Ä¶] MathBERT : a new model [‚Ä¶]\n",
      "13 [‚Ä¶] AI to control aging [‚Ä¶]\n",
      "14 [‚Ä¶] Attention is all you need [‚Ä¶]\n",
      "15 [‚Ä¶] LLaMA 2 has 7B params [‚Ä¶]\n",
      "```\n",
      "\n",
      "We start from a randomly chosen node and then we keep moving to the closest neighbor. \n",
      "\n",
      "#### Navigable Small Worlds: inserting a new vector\n",
      "\n",
      "We can insert a new vector by searching the top KNN with the searching algorithm described before and making an edge between the vector and the top K results.\n",
      "\n",
      "#### HNSW: idea #2\n",
      "\n",
      "To go from NSW (Navigable Small Worlds) to HNSW (Hierarchical Navigable Small Worlds), we need to introduce the algorithm behind the data structure known as Skip-List.\n",
      "\n",
      "The skip list is a data structure that maintains a sorted list and allows search and insertion with an average of ùëÇ(logùëÅ) time complexity.\n",
      "\n",
      "```\n",
      "H3\n",
      "H2\n",
      "H1\n",
      "H0 EEEE\n",
      "1 5 3 9 12 17 23\n",
      "```\n",
      "\n",
      "Let‚Äôs search the number 9\n",
      "\n",
      "#### HNSW: Hierarchical Navigable Small Worlds\n",
      "\n",
      "HNSW uses a hierarchical structure of graphs, where each level is sparser than the previous one.\n",
      "\n",
      "```\n",
      "Q\n",
      "Layer 0 (dense)\n",
      "Layer 1\n",
      "Layer 2\n",
      "Layer 3 (sparse)\n",
      "```\n",
      "\n",
      "Let‚Äôs search!\n",
      "\n",
      "We repeat the search with randomly chosen starting points (on the top layer) and then keep the top K among all the visited nodes.\n",
      "\n",
      "## Thanks for watching!\n",
      "\n",
      "Don‚Äôt forget to subscribe for more amazing content on AI and Machine Learning!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''Your task is to covert the following text into \n",
    "markdown format with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "\n",
    "Also before this first write index in markdown as well with appropriate headings and subheadings.\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Run_pdf.md\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_string_to_markdown(response.text, 'Run_pdf.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction The purpose of this book is t0 describe the optical structur and Oplical properties of the humnan eye_ It Will be useful [0 those who have an inleTeS[ in Vision. such as optometrists, ophthalmologists. vision scientists, Optical physicists. and sludents 0l visual optics.An understanding of the optics of the humaneye is panticularly important designers of ophthalmic diagnostic equipment and visual optical systems. such as telescopes_ Many animals have some SOrt of eye stnucture or sophisticated light sense Like humans. some rely heavily on vision. including predatory birds and insects such as honeybees and dragonflies- However; many animals rely much more on Other senses. particularly hearing and smell . than on vision_ The Visual sense iS Very complex and can process amounts of information Very rapidly How this is done is nOt fully understood: requires greater knowledge of how the neural components of vision (relina_ visual cortex, and other brain centers) process the retinal image. However. the first stage in this complex process is the fonation of the retinal image. In this [ext we investigale how the image is forined and discuss factors that affect its qualily- Most animal eyes can be divided into IWO groups: compound eyes (aS possessed by most insects) and verlebrale eyes (such as the human eye). Compared With Ver- tebrate eyes. there is considerable variation in the compound eyes. Compound eyes cOntain many optical elements (ommatidia). each with own aperture [0 the extemal world  Vertebrate eyes have single aperure [0 the extemal world. which is used by all the detectors. Several other animals have simple eyes which can be described developed versions of the vertebrate eye: All eyes- of whatever type . involve compromises between the need for detection (senSiIvily ) - panticularly at Iow light levels . and resolving capability in terms of the direction or formn of an object Although this book is about the of the human ‚Ç¨ye. we do not wish t0 con- sider the optics in complete isolation from the neural components. othenwvise We canol appreciate what influence changes in the retinal image will have on vision pertormance. As an example. altering the optics considerable influence on Teso- lution of objects for central vision but not for peripheral vision. This because the retina neural structure 1S fine enough at ItS center but not in the penphery- fOr large changes in optical quality to be of imponance (Chapter 18). Thus. the neural components of the visual system particularly the retinal detector. rate some mention in the book: The neural structures of the retina themselves produce optical effects-As an examnple the photoreceptors exhibit waveguide properties that make light ariving from directions more efficient at stimulating vision than light arriving from other directions Another example is that the regular arrangement of the nerve fiber layers produces polanization etiects. While image formnation in the eye is simnilar t0 that in man-made optical systems. such as cumeras and muSt theconventional optical Iaws thete   are   some interesting differences because of the eye biological basis. Perhaps the greatest diffetence iS that. as living organ. the eye responds t0 its environment. often t0 give the best image under different circumstances: Also. grows. suffers huge spatial oplics ha? SOle obey and AEC\n"
     ]
    }
   ],
   "source": [
    "import easyocr\n",
    "\n",
    "def detect_text_with_easyocr(image_path):\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    results = reader.readtext(image_path)\n",
    "    \n",
    "    detected_text = ' '.join([result[1] for result in results])\n",
    "    return detected_text\n",
    "\n",
    "image_path = 'temp.png'\n",
    "text = detect_text_with_easyocr(image_path)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Index\n",
      "\n",
      "### I. Introduction\n",
      "\n",
      "#### 1.1. Purpose of the Book\n",
      "\n",
      "#### 1.2. Importance of Understanding Human Eye Optics\n",
      "\n",
      "#### 1.3. The Visual Sense in Different Animals\n",
      "\n",
      "#### 1.4. Image Formation and Its Quality\n",
      "\n",
      "#### 1.5. Types of Animal Eyes\n",
      "\n",
      "#### 1.6. Compromises in Eye Design\n",
      "\n",
      "#### 1.7. Importance of Neural Components\n",
      "\n",
      "#### 1.8. Optical Effects Produced by Retinal Structures\n",
      "\n",
      "#### 1.9. Differences between Human and Artificial Optical Systems\n",
      "\n",
      "---\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The purpose of this book is to describe the optical structure and optical properties of the human eye. It will be useful to those who have an interest in vision, such as optometrists, ophthalmologists, vision scientists, optical physicists, and students of visual optics. An understanding of the optics of the human eye is particularly important for designers of ophthalmic diagnostic equipment and visual optical systems, such as telescopes. \n",
      "\n",
      "Many animals have some sort of eye structure or sophisticated light sense. Like humans, some rely heavily on vision, including predatory birds and insects such as honeybees and dragonflies. However, many animals rely much more on other senses, particularly hearing and smell, than on vision. \n",
      "\n",
      "The visual sense is very complex and can process vast amounts of information very rapidly. How this is done is not fully understood; it requires greater knowledge of how the neural components of vision (retina, visual cortex, and other brain centers) process the retinal image. However, the first stage in this complex process is the formation of the retinal image. In this text, we investigate how the image is formed and discuss factors that affect its quality. \n",
      "\n",
      "Most animal eyes can be divided into two groups: compound eyes (as possessed by most insects) and vertebrate eyes (such as the human eye). Compared with vertebrate eyes, there is considerable variation in the compound eyes. Compound eyes contain many optical elements (ommatidia), each with its own aperture to the external world. Vertebrate eyes have a single aperture to the external world, which is used by all the detectors. Several other animals have simple eyes which can be described as developed versions of the vertebrate eye.\n",
      "\n",
      "All eyes, of whatever type, involve compromises between the need for detection (sensitivity), particularly at low light levels, and resolving capability in terms of the direction or form of an object. \n",
      "\n",
      "Although this book is about the optics of the human eye, we do not wish to consider the optics in complete isolation from the neural components. Otherwise, we cannot appreciate what influence changes in the retinal image will have on vision performance. As an example, altering the optics has considerable influence on the resolution of objects for central vision but not for peripheral vision. This is because the retina's neural structure is fine enough at its center but not in the periphery for large changes in optical quality to be of importance (Chapter 18). Thus, the neural components of the visual system, particularly the retinal detector, rate some mention in the book.\n",
      "\n",
      "The neural structures of the retina themselves produce optical effects. As an example, the photoreceptors exhibit waveguide properties that make light arriving from certain directions more efficient at stimulating vision than light arriving from other directions. Another example is that the regular arrangement of the nerve fiber layers produces polarization effects. \n",
      "\n",
      "While image formation in the eye is similar to that in man-made optical systems, such as cameras, and must obey the conventional optical laws, there are some interesting differences because of the eye's biological basis. Perhaps the greatest difference is that, as a living organ, the eye responds to its environment, often to give the best image under different circumstances. Also, it grows, suffers huge spatial optics, and has some ability to adapt.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(query + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String successfully saved to Run_image.md\n"
     ]
    }
   ],
   "source": [
    "save_string_to_markdown(response.text, 'Run_image.md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\documentclass[conference]{IEEEtran}\n",
      "\\usepackage[utf8]{inputenc}\n",
      "\\usepackage{amsmath}   % For mathematical formulas\n",
      "\\usepackage{graphicx}  % For including graphics\n",
      "\\usepackage{cite}      % For bibliography management\n",
      "\\usepackage{hyperref}  % For hyperlinks\n",
      "\n",
      "\\title{Report on Command-Line Interface (CLI) Shell Implementation in C}\n",
      "\\author{\\IEEEauthorblockN{Your Name}\n",
      "\\IEEEauthorblockA{Your Institution\\\\\n",
      "Your Email}\n",
      "}\n",
      "\n",
      "\\begin{document}\n",
      "\n",
      "\\maketitle\n",
      "\n",
      "\\begin{abstract}\n",
      "    This C program implements a simple command-line interface (CLI) shell for Windows, similar to popular Unix/Linux shells such as Bash. The shell processes user input, manages command history, changes directories, lists files, and performs basic file system operations. The primary objective of this shell is to allow users to execute basic commands (cd, ls, pwd, echo, hist) and interact with the Windows operating system through a terminal interface.\n",
      "\\end{abstract}\n",
      "\n",
      "\\begin{IEEEkeywords}\n",
      "    CLI, shell, C, Windows, command history, directory navigation, file listing\n",
      "\\end{IEEEkeywords}\n",
      "\n",
      "\\section{Introduction}\n",
      "This document presents a report on the implementation of a simple command-line interface (CLI) shell in C for the Windows operating system. The shell provides basic functionality such as command history, directory navigation, file listing, and the execution of simple commands like cd, ls, pwd, echo, and hist. The primary objective of this project is to familiarize oneself with the fundamental concepts of shell programming and interacting with the Windows API. \n",
      "\n",
      "\\section{Program Structure and Components}\n",
      "\\subsection{Libraries and Macros}\n",
      "The program utilizes various standard C libraries and defines several macros for enhanced code organization. The libraries included are:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{stdio.h}: Provides standard input/output functions.\n",
      "\\item \\texttt{stdlib.h}: Offers standard library functions such as \\texttt{getenv()} and \\texttt{strtok()}.\n",
      "\\item \\texttt{string.h}: Provides functions for string manipulation, including \\texttt{strncpy()} and \\texttt{strcmp()}.\n",
      "\\item \\texttt{windows.h}: Contains Windows-specific system calls, like \\texttt{GetComputerName()} and \\texttt{FindFirstFile()}.\n",
      "\\item \\texttt{direct.h}: Includes directory-related functions such as \\texttt{\\_getcwd()} and \\texttt{\\_chdir()}.\n",
      "\\item \\texttt{limits.h}: Defines constants like \\texttt{PATH\\_MAX} (maximum allowable path length).\n",
      "\\end{itemize}\n",
      "\n",
      "The program defines the following macros:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{MAX\\_INPUT}: Sets the maximum size for user input (1024 characters).\n",
      "\\item \\texttt{MAX\\_ARGS}: Determines the maximum number of command arguments (64).\n",
      "\\item \\texttt{MAX\\_HISTORY}: Defines the size of the command history buffer (10).\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Command History Management}\n",
      "The program maintains a history of the last 10 commands executed by the user. This functionality is implemented using global variables and functions:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\textbf{Global Variables:}\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{history[MAX\\_HISTORY][MAX\\_INPUT]}:  A 2-dimensional array storing the history of the last 10 commands.\n",
      "\\item \\texttt{history\\_count}:  Keeps track of the current number of commands in the history.\n",
      "\\end{itemize}\n",
      "\\item \\textbf{Functions:}\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{add\\_to\\_history(const char *command)}: Adds the latest command to the history. If the history buffer is full, the oldest command is removed, and the new command is added.\n",
      "\\item \\texttt{show\\_history()}: Displays the command history to the user.\n",
      "\\end{itemize}\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Directory Navigation}\n",
      "The shell provides functionality for changing and printing the current directory. This is accomplished through the following functions:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{change\\_directory(char *path, char *prev\\_dir)}: Changes the current working directory. If no path is provided or the user specifies \"~\", it defaults to the user's home directory. If the user enters \"-\", the program switches to the previous directory.\n",
      "\\item \\texttt{print\\_working\\_directory()}: Prints the current working directory using \\texttt{\\_getcwd()}.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{File Listing}\n",
      "The \\texttt{list\\_files()} function lists the files in the current directory using Windows API functions. It leverages \\texttt{FindFirstFile()} and \\texttt{FindNextFile()} to traverse files and directories in the current location.\n",
      "\n",
      "\\subsection{Command Parsing and Execution}\n",
      "The shell reads user input, tokenizes the command, and executes appropriate actions. The input parsing process is as follows:\n",
      "\n",
      "\\begin{enumerate}\n",
      "\\item The shell continuously waits for user input via \\texttt{fgets()}.\n",
      "\\item The input is split into tokens using \\texttt{strtok()}, and each token is stored in the \\texttt{args} array.\n",
      "\\item Commands are matched using \\texttt{strcmp()} and executed based on a set of predefined commands.\n",
      "\\end{enumerate}\n",
      "\n",
      "The following commands are supported:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{cd}: Changes the current directory. Handles special cases like \\texttt{cd -} (switch to the previous directory) and \\texttt{cd ~} (switch to the home directory).\n",
      "\\item \\texttt{ls}: Lists files in the current directory.\n",
      "\\item \\texttt{pwd}: Prints the current working directory.\n",
      "\\item \\texttt{echo}: Prints the provided arguments to the terminal.\n",
      "\\item \\texttt{hist}: Displays the command history.\n",
      "\\item \\texttt{exit}: Exits the shell program.\n",
      "\\end{itemize}\n",
      "\n",
      "\\subsection{Command Execution Workflow}\n",
      "The command execution workflow is as follows:\n",
      "\n",
      "\\begin{enumerate}\n",
      "\\item \\textbf{Prompt Display}: The shell prints a prompt in the format \\texttt{[username@hostname cwd]\\$} where:\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{username}: Retrieved from the environment variable \\texttt{USERNAME}.\n",
      "\\item \\texttt{hostname}: Obtained using the Windows function \\texttt{GetComputerName()}.\n",
      "\\item \\texttt{cwd}: The current working directory, fetched using \\texttt{\\_getcwd()}.\n",
      "\\end{itemize}\n",
      "\\item \\textbf{Input Handling}:\n",
      "\\begin{itemize}\n",
      "\\item The shell reads user input and processes it with \\texttt{fgets()}.\n",
      "\\item If the command is not empty, it is added to the history using \\texttt{add\\_to\\_history()}.\n",
      "\\end{itemize}\n",
      "\\item \\textbf{Command Execution}:\n",
      "\\begin{itemize}\n",
      "\\item The shell tokenizes the input into commands and arguments.\n",
      "\\item It checks for each supported command (cd, ls, pwd, echo, etc.) and invokes the corresponding function.\n",
      "\\item If an unsupported command is entered, the shell prints an error message.\n",
      "\\end{itemize}\n",
      "\\item \\textbf{Looping}: The program continuously loops until the user types \\texttt{exit}.\n",
      "\\end{enumerate}\n",
      "\n",
      "\\section{Error Handling}\n",
      "The shell incorporates basic error handling mechanisms to ensure robust operation. It handles errors such as:\n",
      "\n",
      "\\begin{itemize}\n",
      "\\item If \\texttt{getcwd()} or \\texttt{chdir()} fails, it prints an appropriate error message.\n",
      "\\item If too many arguments are passed to the \\texttt{cd} command, the program prints an error and ignores the command.\n",
      "\\item The \\texttt{FindFirstFile()} function handles cases where the directory cannot be read.\n",
      "\\end{itemize}\n",
      "\n",
      "\\section{Key Functions}\n",
      "\\begin{itemize}\n",
      "\\item \\texttt{change\\_directory(char *path, char *prev\\_dir)}: Handles changing directories and supports navigating to the home directory (~) and switching to the previous directory (-).\n",
      "\\item \\texttt{list\\_files()}: Uses Windows API functions to list files and directories in the current location.\n",
      "\\item \\texttt{echo\\_command(char *input)}: This function prints the user‚Äôs input to the terminal, implementing an echo command.\n",
      "\\end{itemize}\n",
      "\n",
      "\\section{Conclusion}\n",
      "This report has presented a simple command-line shell implemented in C for the Windows operating system. The shell successfully integrates essential features like command history, directory navigation, file listing, and the execution of basic commands. The use of Windows API functions enables interaction with the file system, while standard C functions handle input/output and string manipulation.\n",
      "\n",
      "The shell could be further enhanced by adding support for more commands and features such as piping, file redirection, or job control to make it more powerful and versatile.\n",
      "\\end{document}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "Report on Command-Line Interface (CLI) Shell Implementation in C\n",
    "1. Introduction\n",
    "This C program implements a simple command-line interface (CLI) shell for Windows, similar to popular Unix/Linux shells such as Bash. The shell processes user input, manages command history, changes directories, lists files, and performs basic file system operations. The primary objective of this shell is to allow users to execute basic commands (cd, ls, pwd, echo, hist) and interact with the Windows operating system through a terminal interface.\n",
    "\n",
    "2. Program Structure and Components\n",
    "2.1. Libraries and Macros\n",
    "The program includes several standard libraries:\n",
    "\n",
    "<stdio.h>: Standard input/output functions.\n",
    "<stdlib.h>: Standard library functions like getenv() and strtok().\n",
    "<string.h>: Functions for string manipulation (strncpy(), strcmp()).\n",
    "<windows.h>: Windows-specific system calls (GetComputerName(), FindFirstFile()).\n",
    "<direct.h>: For directory-related functions like _getcwd() and _chdir().\n",
    "<limits.h>: For PATH_MAX (maximum allowable path length).\n",
    "Macros:\n",
    "\n",
    "MAX_INPUT: Defines the maximum size for input (1024 characters).\n",
    "MAX_ARGS: Sets the maximum number of command arguments (64).\n",
    "MAX_HISTORY: Sets the size of the command history buffer (10).\n",
    "2.2. Command History Management\n",
    "The program maintains a history of the last 10 commands executed by the user.\n",
    "\n",
    "Global Variables:\n",
    "history[MAX_HISTORY][MAX_INPUT]: Stores the history of the last 10 commands.\n",
    "history_count: Tracks the current number of commands in the history.\n",
    "Functions:\n",
    "\n",
    "add_to_history(const char *command): Adds the latest command to the history. If the history buffer is full, the oldest command is removed, and the new command is added.\n",
    "show_history(): Displays the command history to the user.\n",
    "2.3. Directory Navigation\n",
    "The program provides functionality to change and print the current directory.\n",
    "\n",
    "Functions:\n",
    "\n",
    "change_directory(char *path, char *prev_dir): Changes the current working directory. If no path is provided or the user specifies \"~\", it defaults to the user's home directory. If the user enters \"-\", the program switches to the previous directory.\n",
    "print_working_directory(): Prints the current working directory using _getcwd().\n",
    "2.4. File Listing\n",
    "list_files(): Lists the files in the current directory using Windows API functions. It leverages FindFirstFile() and FindNextFile() to traverse files and directories in the current location.\n",
    "2.5. Command Parsing and Execution\n",
    "The shell reads user input, tokenizes the command, and executes appropriate actions.\n",
    "\n",
    "Input Parsing:\n",
    "\n",
    "The shell continuously waits for user input via fgets().\n",
    "The input is split into tokens using strtok(), and each token is stored in the args array.\n",
    "Commands are matched using strcmp() and executed based on a set of predefined commands.\n",
    "Supported Commands:\n",
    "\n",
    "cd: Changes the current directory. Handles special cases like cd - (switch to the previous directory) and cd ~ (switch to the home directory).\n",
    "ls: Lists files in the current directory.\n",
    "pwd: Prints the current working directory.\n",
    "echo: Prints the provided arguments to the terminal.\n",
    "hist: Displays the command history.\n",
    "exit: Exits the shell program.\n",
    "2.6. Command Execution Workflow\n",
    "Prompt Display: The shell prints a prompt in the format [username@hostname cwd]$, where:\n",
    "\n",
    "username: Retrieved from the environment variable USERNAME.\n",
    "hostname: Obtained using the Windows function GetComputerName().\n",
    "cwd: The current working directory, fetched using _getcwd().\n",
    "Input Handling:\n",
    "\n",
    "The shell reads user input and processes it with fgets().\n",
    "If the command is not empty, it is added to the history using add_to_history().\n",
    "Command Execution:\n",
    "\n",
    "The shell tokenizes the input into commands and arguments.\n",
    "It checks for each supported command (cd, ls, pwd, echo, etc.) and invokes the corresponding function.\n",
    "If an unsupported command is entered, the shell prints an error message.\n",
    "Looping: The program continuously loops until the user types exit.\n",
    "\n",
    "3. Error Handling\n",
    "The shell performs basic error handling:\n",
    "\n",
    "If getcwd() or chdir() fails, it prints an appropriate error message.\n",
    "If too many arguments are passed to the cd command, the program prints an error and ignores the command.\n",
    "The FindFirstFile() function handles cases where the directory cannot be read.\n",
    "4. Key Functions\n",
    "change_directory(char *path, char *prev_dir): Handles changing directories and supports navigating to the home directory (~) and switching to the previous directory (-).\n",
    "\n",
    "list_files(): Uses Windows API functions to list files and directories in the current location.\n",
    "\n",
    "echo_command(char *input): This function prints the user‚Äôs input to the terminal, implementing an echo command.\n",
    "\n",
    "5. Conclusion\n",
    "This program demonstrates how a simple command-line shell can be implemented in C on a Windows system. It includes key features like command history, directory navigation, file listing, and the ability to execute basic commands. The use of Windows API functions enables interaction with the file system, while standard C functions handle input/output and string manipulation.\n",
    "\n",
    "The shell could be further extended by adding support for more commands and features like piping, file redirection, or job control to make it more powerful.'''\n",
    "\n",
    "query = r'''\n",
    "Your task is to covert the following text into simple Latex code with appropriate headings, section etc.\n",
    "Make sure to do the appropriate formatting.\n",
    "Keep in mind to account for speacial characters such as:\n",
    "# should be \\#\n",
    "$ should be \\$\n",
    "% should be \\%\n",
    "& should be \\&\n",
    "_ should be \\_\n",
    "\n",
    "and so on.\n",
    "\n",
    "\n",
    "\n",
    "Make sure to use only ascii characters for the latex code.\n",
    "\n",
    "Also just directly give the latex code, nothing else should be included in output.\n",
    "This is the template you can use:\n",
    "'''\n",
    "\n",
    "template = r'''\n",
    "Use the following template:\n",
    "\\documentclass[conference]{IEEEtran}\n",
    "\\usepackage[utf8]{inputenc}\n",
    "\\usepackage{amsmath}   % For mathematical formulas\n",
    "\\usepackage{graphicx}  % For including graphics\n",
    "\\usepackage{cite}      % For bibliography management\n",
    "\\usepackage{hyperref}  % For hyperlinks\n",
    "\n",
    "\\title{Title of the Research Paper}\n",
    "\\author{\\IEEEauthorblockN{Your Name}\n",
    "\\IEEEauthorblockA{Your Institution\\\\\n",
    "Your Email}\n",
    "}\n",
    "\n",
    "\\begin{document}\n",
    "\n",
    "\\maketitle\n",
    "\n",
    "\\begin{abstract}\n",
    "    This is a brief summary of the research paper, providing an overview of the main objectives, methodology, results, and conclusions. The abstract should be concise, usually around 150-250 words.\n",
    "\\end{abstract}\n",
    "\n",
    "\\begin{IEEEkeywords}\n",
    "    Keyword1, Keyword2, Keyword3, Keyword4\n",
    "\\end{IEEEkeywords}\n",
    "\n",
    "\\section{Introduction}\n",
    "The introduction provides background information on the topic of the research, outlines the problem being addressed, and states the objectives of the study. It may also include a brief literature review.\n",
    "\n",
    "\\section{Methodology}\n",
    "In this section, describe the methods used in your research. Include information about the experimental design, materials, procedures, and any statistical analyses performed.\n",
    "\n",
    "\\section{Results}\n",
    "Present the findings of your research. Use tables, figures, and graphs where necessary to illustrate your results. Each table or figure should be accompanied by a caption explaining its contents.\n",
    "\n",
    "\\subsection{Subsection Title}\n",
    "If needed, you can include subsections to further organize your results.\n",
    "\n",
    "\\section{Discussion}\n",
    "Interpret the results in this section. Discuss their implications, limitations, and how they relate to previous research. Consider addressing any unexpected findings.\n",
    "\n",
    "\\section{Conclusion}\n",
    "Summarize the main findings and contributions of your research. You may also suggest directions for future research.\n",
    "\n",
    "\\section*{Acknowledgments}\n",
    "Acknowledge any individuals or organizations that contributed to your research but do not meet the criteria for authorship.\n",
    "\n",
    "\\bibliographystyle{IEEEtran}  % IEEE bibliography style\n",
    "\\bibliography{references}       % Include your bibliography file (references.bib)\n",
    "\n",
    "\\end{document}\n",
    "\n",
    "Here is the text:\n",
    "'''\n",
    "\n",
    "response = model.generate_content(query + template + text)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def string_to_tex_file(latex_string, filename):\n",
    "    \"\"\"Convert a LaTeX string to a .tex file.\"\"\"\n",
    "\n",
    "    with open(filename, 'w') as tex_file:\n",
    "        tex_file.write(latex_string)\n",
    "    print(f\"Written LaTeX content to {filename}\")\n",
    "\n",
    "def compile_tex_to_pdf(tex_filename):\n",
    "    \"\"\"Compile the .tex file to a PDF using pdflatex.\"\"\"\n",
    "    try:\n",
    "        # Use subprocess to call pdflatex and automatically respond to input requests\n",
    "        process = subprocess.Popen(\n",
    "            ['pdflatex', tex_filename],\n",
    "            stdin=subprocess.PIPE,   # Allow sending input to the process\n",
    "            stdout=subprocess.PIPE,   # Capture standard output\n",
    "            stderr=subprocess.PIPE,   # Capture standard error\n",
    "            text=True                 # Use text mode for input/output\n",
    "        )\n",
    "        \n",
    "        # Automatically send an Enter key press whenever it expects input\n",
    "        stdout, stderr = process.communicate(input='\\n')  # Sending newline (Enter)\n",
    "        \n",
    "        # Check the return code of the process\n",
    "        if process.returncode == 0:\n",
    "            print(f\"Successfully compiled {tex_filename} to PDF.\")\n",
    "        else:\n",
    "            print(f\"Error compiling {tex_filename} to PDF.\")\n",
    "            print(\"STDOUT:\", stdout)\n",
    "            print(\"STDERR:\", stderr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written LaTeX content to report.tex\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the filename for the .tex file\n",
    "tex_filename = 'report.tex'\n",
    "\n",
    "# Convert the string to a .tex file\n",
    "string_to_tex_file(response.text, tex_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error compiling report.tex to PDF.\n",
      "STDOUT: This is pdfTeX, Version 3.141592653-2.6-1.40.25 (MiKTeX 24.1) (preloaded format=pdflatex.fmt)\n",
      " restricted \\write18 enabled.\n",
      "entering extended mode\n",
      "(report.tex\n",
      "LaTeX2e <2023-11-01> patch level 1\n",
      "L3 programming layer <2024-01-04>\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/ieeetran\\IEEEtran.cls\n",
      "Document Class: IEEEtran 2015/08/26 V1.8b by Michael Shell\n",
      "-- See the \"IEEEtran_HOWTO\" manual for usage information.\n",
      "-- http://www.michaelshell.org/tex/ieeetran/\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/psnfss\\ot1ptm.fd)\n",
      "-- Using 8.5in x 11in (letter) paper.\n",
      "-- Using PDF output.\n",
      "-- This is a 10 point document.\n",
      ") (C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/base\\inputenc.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/amsmath\\amsmath.sty\n",
      "For additional information on amsmath, use the `?' option.\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/amsmath\\amstext.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/amsmath\\amsgen.sty))\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/amsmath\\amsbsy.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/amsmath\\amsopn.sty))\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/graphics\\graphicx.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/graphics\\keyval.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/graphics\\graphics.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/graphics\\trig.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/graphics-cfg\\graphics.c\n",
      "fg)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/graphics-def\\pdftex.def\n",
      "))) (C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/cite\\cite.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/hyperref\\hyperref.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/iftex\\iftex.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/infwarerr\\infwarerr.s\n",
      "ty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/kvsetkeys\\kvsetkeys.sty\n",
      ")\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/kvdefinekeys\\kvdefine\n",
      "keys.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/pdfescape\\pdfescape.s\n",
      "ty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/ltxcmds\\ltxcmds.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/pdftexcmds\\pdftexcmds\n",
      ".sty))\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/hycolor\\hycolor.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/letltxmacro\\letltxmacro\n",
      ".sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/auxhook\\auxhook.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/hyperref\\nameref.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/refcount\\refcount.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/gettitlestring\\gettit\n",
      "lestring.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/kvoptions\\kvoptions.sty\n",
      ")))\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/etoolbox\\etoolbox.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/hyperref\\pd1enc.def)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/intcalc\\intcalc.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/hyperref\\puenc.def)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/url\\url.sty)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/bitset\\bitset.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/bigintcalc\\bigintcalc\n",
      ".sty))\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/base\\atbegshi-ltx.sty))\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/hyperref\\hpdftex.def\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/base\\atveryend-ltx.sty)\n",
      "\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/rerunfilecheck\\rerunfil\n",
      "echeck.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/generic/uniquecounter\\uniquec\n",
      "ounter.sty)))\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/l3backend\\l3backend-pdf\n",
      "tex.def) (report.aux)\n",
      "-- Lines per column: 56 (exact).\n",
      "\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/context/base/mkii\\supp-pdf.mk\n",
      "ii\n",
      "[Loading MPS to PDF converter (version 2006.09.02).]\n",
      ")\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/epstopdf-pkg\\epstopdf-b\n",
      "ase.sty\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/00miktex\\epstopdf-sys.c\n",
      "fg)) (report.out) (report.out)\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/psnfss\\ot1pcr.fd)\n",
      "Underfull \\hbox (badness 10000) in paragraph at lines 61--62\n",
      "[]\\OT1/pcr/m/n/10 add_to_history(const char *command)\\OT1/ptm/m/n/10 :\n",
      "\n",
      "Underfull \\hbox (badness 10000) in paragraph at lines 70--71\n",
      "[]\\OT1/pcr/m/n/10 change_directory(char *path, char\n",
      "\n",
      "Underfull \\hbox (badness 3679) in paragraph at lines 81--82\n",
      "[]\\OT1/ptm/m/n/10 The shell con-tin-u-ously waits for user in-put via\n",
      "[1{C:/Users/nagar/AppData/Local/MiKTeX/fonts/map/pdftex/pdftex.map}{C:/Users/na\n",
      "gar/AppData/Local/Programs/MiKTeX/fonts/enc/dvips/base/8r.enc}]\n",
      "(C:\\Users\\nagar\\AppData\\Local\\Programs\\MiKTeX\\tex/latex/psnfss\\ts1pcr.fd)\n",
      "Underfull \\hbox (badness 10000) in paragraph at lines 132--133\n",
      "[]\\OT1/pcr/m/n/10 change_directory(char *path, char\n",
      "\n",
      "Underfull \\hbox (badness 10000) in paragraph at lines 132--133\n",
      "\\OT1/pcr/m/n/10 *prev_dir)\\OT1/ptm/m/n/10 : Han-dles chang-ing di-rec-to-ries a\n",
      "nd\n",
      "\n",
      "Underfull \\hbox (badness 1354) in paragraph at lines 132--133\n",
      "\\OT1/ptm/m/n/10 sup-ports nav-i-gat-ing to the home di-rec-tory ( ) and\n",
      "\n",
      "! LaTeX Error: Invalid UTF-8 byte \"92.\n",
      "\n",
      "See the LaTeX manual or LaTeX Companion for explanation.\n",
      "Type  H <return>  for immediate help.\n",
      " ...                                              \n",
      "                                                  \n",
      "l.134 ... *input)}: This function prints the user‚Äô\n",
      "                                                  s input to the terminal, i...\n",
      "\n",
      "? \n",
      "Underfull \\hbox (badness 10000) in paragraph at lines 134--135\n",
      "[]\\OT1/pcr/m/n/10 echo_command(char *input)\\OT1/ptm/m/n/10 : This func-tion\n",
      "\n",
      "** Conference Paper **\n",
      "Before submitting the final camera ready copy, remember to:\n",
      "\n",
      " 1. Manually equalize the lengths of two columns on the last page\n",
      " of your paper;\n",
      "\n",
      " 2. Ensure that any PostScript and/or PDF output post-processing\n",
      " uses only Type 1 fonts and that every step in the generation\n",
      " process uses the appropriate paper size.\n",
      "\n",
      "[2] (report.aux) )<C:/Users/nagar/AppData/Local/Programs/MiKTeX/fonts/type1/pub\n",
      "lic/amsfonts/cm/cmsy7.pfb><C:/Users/nagar/AppData/Local/Programs/MiKTeX/fonts/t\n",
      "ype1/urw/courier/ucrr8a.pfb><C:/Users/nagar/AppData/Local/Programs/MiKTeX/fonts\n",
      "/type1/urw/times/utmb8a.pfb><C:/Users/nagar/AppData/Local/Programs/MiKTeX/fonts\n",
      "/type1/urw/times/utmbi8a.pfb><C:/Users/nagar/AppData/Local/Programs/MiKTeX/font\n",
      "s/type1/urw/times/utmr8a.pfb><C:/Users/nagar/AppData/Local/Programs/MiKTeX/font\n",
      "s/type1/urw/times/utmri8a.pfb>\n",
      "Output written on report.pdf (2 pages, 96428 bytes).\n",
      "Transcript written on report.log.\n",
      "\n",
      "STDERR: pdflatex: security risk: running with elevated privileges\n",
      "pdflatex: major issue: So far, you have not checked for MiKTeX updates.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tex_filename = 'report.tex'\n",
    "compile_tex_to_pdf(tex_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doc_form",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
